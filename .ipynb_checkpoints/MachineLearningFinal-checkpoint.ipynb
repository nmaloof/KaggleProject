{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# Importing Libraries #\n",
    "#######################\n",
    "\n",
    "#--Adding Data Types--#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#--Processing--#\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#--RandomForest--#\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#--Gradient Boosting--#\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#--Extreme Gradient Boosting--#\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#--Linear ElasticNet Regression--#\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "#--Pipeline For Stacking--#\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#--Error Metric--#\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#--Optimization--#\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "#--Visualizations--#\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation For Tree's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dimensions:  (1460, 65)\n",
      "Testing Dimensions:  (1459, 64)\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# Loading the Data #\n",
    "####################\n",
    "\n",
    "train_clean = pd.read_csv(\"train_clean.csv\")\n",
    "test_clean = pd.read_csv(\"test_clean.csv\")\n",
    "\n",
    "print(\"Training Dimensions: \", train_clean.shape)\n",
    "print(\"Testing Dimensions: \", test_clean.shape)\n",
    "\n",
    "######################\n",
    "# Getting Id Columns #\n",
    "######################\n",
    "colId = pd.read_csv(\"test.csv\")\n",
    "colId = colId.Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "# Applying Transforms to Functions #\n",
    "####################################\n",
    "\n",
    "train_clean['SalePrice'] = train_clean['SalePrice'].apply(lambda x: np.log(x + 1))\n",
    "train_clean['GarageArea'] = train_clean['GarageArea'].apply(lambda x: np.log(x + 1))\n",
    "train_clean['X2ndFlrSF'] = train_clean['X2ndFlrSF'].apply(lambda x: np.log(x + 1))\n",
    "train_clean['TotalBsmtSF'] = train_clean['TotalBsmtSF'].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "test_clean['GarageArea'] = test_clean['GarageArea'].apply(lambda x: np.log(x + 1))\n",
    "test_clean['X2ndFlrSF'] = test_clean['X2ndFlrSF'].apply(lambda x: np.log(x + 1))\n",
    "test_clean['TotalBsmtSF'] = test_clean['TotalBsmtSF'].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "\n",
    "\n",
    "for c in train_clean.columns:\n",
    "    if train_clean[c].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        # Need to convert the column type to string in order to encode missing values\n",
    "        train_clean[c] = le.fit_transform(train_clean[c].astype(str))\n",
    "for c in test_clean.columns:\n",
    "    if test_clean[c].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        # Need to convert the column type to string in order to encode missing values\n",
    "        test_clean[c] = le.fit_transform(test_clean[c].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################\n",
    "# # Altering the DataSet # (These changes are based on the feature importances by the models, to be commented out later)\n",
    "# ########################\n",
    "\n",
    "# # Random Forest Features #\n",
    "# ##########################\n",
    "# train_clean.drop(rf_lowest_features, axis = 1, inplace = True)\n",
    "# test_clean.drop(rf_lowest_features, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1168, 65)\n",
      "(292, 65)\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# Splitting Data #  #(Only splitting the training data into two more sets called train_set, and test_set)\n",
    "##################\n",
    "\n",
    "train_set, test_set = train_test_split(train_clean, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)\n",
    "\n",
    "X_train = train_set.drop(\"SalePrice\", axis = 1)\n",
    "Y_train = train_set.SalePrice\n",
    "\n",
    "X_test = test_set.drop(\"SalePrice\", axis = 1)\n",
    "Y_test = test_set.SalePrice\n",
    "\n",
    "#########################################\n",
    "# The Full Original Training Set to Use #\n",
    "#########################################\n",
    "\n",
    "X_full_train = train_clean.drop(\"SalePrice\", axis = 1)\n",
    "Y_full_train = train_clean.SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# RandomForest Model To See Best Features Split #\n",
    "#################################################\n",
    "mse = []\n",
    "for i in range(1,65):\n",
    "    randForest = RandomForestRegressor(n_estimators=1000, min_samples_leaf= 5, \n",
    "                                       max_features=i, oob_score = True, random_state=42, n_jobs=3)\n",
    "    randForest.fit(X_train, Y_train)\n",
    "    forestPredictions = randForest.predict(X_test)\n",
    "    mse.append(mean_squared_error(Y_test, forestPredictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 : 0.149965853034\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Just to See The Index of the Lowest Tree #\n",
    "############################################\n",
    "lowest = 100000\n",
    "index = 100000\n",
    "for i,k in enumerate(mse):\n",
    "    if k < lowest:\n",
    "        lowest = k\n",
    "        index = i\n",
    "print(index, ':', lowest**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=48, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=5,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10000, n_jobs=3, oob_score=True, random_state=42,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################################\n",
    "# Running the Forest on The Whole Training #\n",
    "############################################\n",
    "\n",
    "randForest = RandomForestRegressor(n_estimators=10000, min_samples_leaf= 5, \n",
    "                                       max_features=48, oob_score = True, random_state=42, n_jobs=3)\n",
    "randForest.fit(X_full_train, Y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[122753.72779751537, 152625.27336808175, 178481.14250168705, 182418.27386204046, 197268.41760919621]\n"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "# Predicting The Kaggle DataSet with RandomForest #\n",
    "###################################################\n",
    "\n",
    "KagglePredictions = randForest.predict(test_clean)\n",
    "KagglePredictions = [np.exp(x) - 1 for x in KagglePredictions]\n",
    "pd.DataFrame({\"SalePrice\":KagglePredictions, \"Id\": colId}).to_csv(\"KaggleSubmitPythonForest.csv\", index = False)\n",
    "print(KagglePredictions[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# Setting Up Gradient Boosting #\n",
    "################################\n",
    "\n",
    "def gradBoostCV(n_estimators, max_depth, max_features, min_samples_leaf):\n",
    "    val = cross_val_score(GradientBoostingRegressor(\n",
    "    n_estimators = int(n_estimators), max_depth = int(max_depth), max_features = int(max_features), min_samples_leaf = int(min_samples_leaf),\n",
    "        random_state = 42, learning_rate = 0.05\n",
    "    ),X_train, Y_train, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = 3).mean()\n",
    "    return val\n",
    "\n",
    "gradBoostBaye = BayesianOptimization(gradBoostCV, {\n",
    "    'n_estimators': (100, 10000),\n",
    "    'max_depth': (1,15),\n",
    "    \"max_features\": (1,65),\n",
    "    'min_samples_leaf': (5,10)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m--------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   max_depth |   max_features |   min_samples_leaf |   n_estimators | \n",
      "    1 | 00m25s | \u001b[35m  -0.01740\u001b[0m | \u001b[32m     2.1213\u001b[0m | \u001b[32m       40.5518\u001b[0m | \u001b[32m            6.2855\u001b[0m | \u001b[32m     7205.1311\u001b[0m | \n",
      "    2 | 00m51s |   -0.01879 |     13.0163 |        56.9100 |             9.7723 |      2542.4709 | \n",
      "    3 | 00m57s | \u001b[35m  -0.01729\u001b[0m | \u001b[32m     3.9606\u001b[0m | \u001b[32m       58.0786\u001b[0m | \u001b[32m            7.5573\u001b[0m | \u001b[32m     7237.1089\u001b[0m | \n",
      "    4 | 00m19s |   -0.01741 |     11.6052 |        26.0721 |             7.4422 |      4181.5886 | \n",
      "    5 | 00m48s | \u001b[35m  -0.01635\u001b[0m | \u001b[32m     3.7100\u001b[0m | \u001b[32m       39.5609\u001b[0m | \u001b[32m            9.2472\u001b[0m | \u001b[32m     1846.7542\u001b[0m | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m--------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   max_depth |   max_features |   min_samples_leaf |   n_estimators | \n",
      "    6 | 00m29s |   -0.01646 |      5.2036 |         8.0318 |             5.6705 |      9999.1808 | \n",
      "    7 | 00m12s |   -0.01931 |     13.8096 |        61.6394 |             9.4293 |       108.0693 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  1.02791062e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 48, 'nit': 4, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    8 | 00m27s |   -0.01853 |      1.7532 |        64.8107 |             5.9101 |      8874.6536 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.25596998e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 48, 'nit': 5, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-0.00013475]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 47, 'nit': 4, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -5.80981214e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 56, 'nit': 5, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9 | 00m20s |   -0.01831 |      1.1025 |        62.0518 |             9.5330 |      5645.6677 | \n",
      "   10 | 00m10s |   -0.01739 |     14.9063 |         2.9205 |             9.8321 |      1236.1833 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-0.00010789]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 60, 'nit': 6, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   11 | 00m18s |   -0.01677 |      3.2369 |        64.5562 |             5.9543 |      1129.5692 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   12 | 00m57s |   -0.01796 |      5.7876 |        63.6899 |             6.5912 |      9992.3363 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   13 | 00m21s |   -0.01777 |     13.6154 |         1.1002 |             8.8027 |      7961.5180 | \n",
      "   14 | 00m46s |   -0.02151 |     14.1591 |        64.8283 |             5.0075 |      1655.7235 | \n",
      "   15 | 00m10s |   -0.01789 |      1.0866 |         3.3231 |             6.5536 |      3425.0857 | \n",
      "   16 | 00m08s |   -0.02067 |      1.3507 |         6.5466 |             9.7074 |       407.7130 | \n",
      "   17 | 00m15s | \u001b[35m  -0.01623\u001b[0m | \u001b[32m     3.4541\u001b[0m | \u001b[32m        4.1313\u001b[0m | \u001b[32m            9.0898\u001b[0m | \u001b[32m     4955.7861\u001b[0m | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ 0.00012216]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 46, 'nit': 3, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   18 | 00m20s |   -0.01759 |      9.5853 |         1.3869 |             9.8135 |      9543.8032 | \n",
      "   19 | 00m10s |   -0.01917 |      1.5673 |         1.1998 |             6.0771 |      2327.1839 | \n",
      "   20 | 00m18s |   -0.01831 |      1.0112 |        63.9834 |             5.3516 |      4625.3266 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ 0.0003393]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 50, 'nit': 4, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   21 | 00m18s |   -0.01679 |     13.9870 |         7.3976 |             9.9832 |      6469.4554 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ 0.0003227]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 50, 'nit': 5, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -4.59994353e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 51, 'nit': 5, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   22 | 00m37s |   -0.01708 |      3.6401 |        59.0221 |             8.8821 |      3719.5245 | \n",
      "   23 | 00m18s |   -0.01677 |     14.6404 |         3.1853 |             9.5535 |      8654.3414 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -3.76629239e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 49, 'nit': 5, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   24 | 00m25s |   -0.01832 |      1.2150 |        57.0642 |             9.9639 |      8351.6868 | \n",
      "   25 | 00m11s |   -0.01838 |      1.8591 |         1.9962 |             9.6457 |      4383.9591 | \n",
      "   26 | 00m16s |   -0.01837 |      1.1332 |        57.7116 |             9.9691 |      2977.2795 | \n",
      "   27 | 01m09s |   -0.01918 |     14.4548 |        63.5905 |             9.5972 |      5217.6080 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-13653431.6268782]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 34, 'nit': 2, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-31773068.82497382]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  3.02586138e+12]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   28 | 00m15s |   -0.01781 |     13.9259 |         1.2197 |             9.7811 |      5966.5138 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.04936309e+14]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-6139210.68967056]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 35, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.94248937e+22]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-1122116.56455648]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 22, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-12068800.75624084]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 33, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-11404491.5279808]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 32, 'nit': 2, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -3.91125914e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   29 | 00m10s |   -0.01678 |     14.0144 |         5.8646 |             9.1977 |       823.8365 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-3828950.70342433]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 34, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  1.05357735e+16]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -4.22613750e+12]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-11742434.74215269]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 34, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-9513290.64884138]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 34, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-1408764.11842115]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  2.44305801e+17]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   30 | 00m07s |   -0.02062 |      1.0000 |         1.0000 |            10.0000 |      1029.0211 | \n"
     ]
    },
    {
     "ename": "JoblibValueError",
     "evalue": "JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/runpy.py in _run_code(code=<code object <module> at 0x10497a6f0, file \"/Use...3.6/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/Users/nicholasmaloof/anaconda/lib/python3.6/sit...ges/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/Users/nicho.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x10497a6f0, file \"/Use...3.6/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/Users/nicholasmaloof/anaconda/lib/python3.6/sit...ges/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/Users/nicho.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    472             return self.subapp.start()\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         try:\n--> 477             ioloop.IOLoop.instance().start()\n    478         except KeyboardInterrupt:\n    479             pass\n    480 \n    481 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'gradBoostBaye.maximize(n_iter=30)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2017, 11, 11, 18, 40, 55, 248959, tzinfo=tzutc()), 'msg_id': '74E4C6630D7D4B8D896F0A0982EC0450', 'msg_type': 'execute_request', 'session': '290CB1C368FE44B4B910F9565602A42B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '74E4C6630D7D4B8D896F0A0982EC0450', 'msg_type': 'execute_request', 'parent_header': {}})\n    230             self.log.warn(\"Unknown message type: %r\", msg_type)\n    231         else:\n    232             self.log.debug(\"%s: %s\", msg_type, msg)\n    233             self.pre_handler_hook()\n    234             try:\n--> 235                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'290CB1C368FE44B4B910F9565602A42B']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'gradBoostBaye.maximize(n_iter=30)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2017, 11, 11, 18, 40, 55, 248959, tzinfo=tzutc()), 'msg_id': '74E4C6630D7D4B8D896F0A0982EC0450', 'msg_type': 'execute_request', 'session': '290CB1C368FE44B4B910F9565602A42B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '74E4C6630D7D4B8D896F0A0982EC0450', 'msg_type': 'execute_request', 'parent_header': {}}\n    236             except Exception:\n    237                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    238             finally:\n    239                 self.post_handler_hook()\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'290CB1C368FE44B4B910F9565602A42B'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'gradBoostBaye.maximize(n_iter=30)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2017, 11, 11, 18, 40, 55, 248959, tzinfo=tzutc()), 'msg_id': '74E4C6630D7D4B8D896F0A0982EC0450', 'msg_type': 'execute_request', 'session': '290CB1C368FE44B4B910F9565602A42B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '74E4C6630D7D4B8D896F0A0982EC0450', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='gradBoostBaye.maximize(n_iter=30)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'gradBoostBaye.maximize(n_iter=30)'\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('gradBoostBaye.maximize(n_iter=30)',), **kwargs={'silent': False, 'store_history': True})\n    528             )\n    529         self.payload_manager.write_payload(payload)\n    530 \n    531     def run_cell(self, *args, **kwargs):\n    532         self._last_traceback = None\n--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('gradBoostBaye.maximize(n_iter=30)',)\n        kwargs = {'silent': False, 'store_history': True}\n    534 \n    535     def _showtraceback(self, etype, evalue, stb):\n    536         # try to preserve ordering of tracebacks and print statements\n    537         sys.stdout.flush()\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='gradBoostBaye.maximize(n_iter=30)', store_history=True, silent=False, shell_futures=True)\n   2693                 self.displayhook.exec_result = result\n   2694 \n   2695                 # Execute the user code\n   2696                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2697                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2698                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2699                 \n   2700                 self.last_execution_succeeded = not has_raised\n   2701 \n   2702                 # Reset this so later displayed values do not modify the\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Expr object>], cell_name='<ipython-input-13-1608e59afc68>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 10cf7c358, execution_..._before_exec=None error_in_exec=None result=None>)\n   2803                     return True\n   2804 \n   2805             for i, node in enumerate(to_run_interactive):\n   2806                 mod = ast.Interactive([node])\n   2807                 code = compiler(mod, cell_name, \"single\")\n-> 2808                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x10bbeae40, file \"<ipython-input-13-1608e59afc68>\", line 1>\n        result = <ExecutionResult object at 10cf7c358, execution_..._before_exec=None error_in_exec=None result=None>\n   2809                     return True\n   2810 \n   2811             # Flush softspace\n   2812             if softspace(sys.stdout, 0):\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x10bbeae40, file \"<ipython-input-13-1608e59afc68>\", line 1>, result=<ExecutionResult object at 10cf7c358, execution_..._before_exec=None error_in_exec=None result=None>)\n   2857         outflag = True  # happens in more places, so it's easier as default\n   2858         try:\n   2859             try:\n   2860                 self.hooks.pre_run_code_hook()\n   2861                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2862                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x10bbeae40, file \"<ipython-input-13-1608e59afc68>\", line 1>\n        self.user_global_ns = {'BayesianOptimization': <class 'bayes_opt.bayesian_optimization.BayesianOptimization'>, 'ElasticNet': <class 'sklearn.linear_model.coordinate_descent.ElasticNet'>, 'GradientBoostingRegressor': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>, 'In': ['', '#######################\\n# Importing Libraries #\\n...ion--#\\nfrom bayes_opt import BayesianOptimization', \"pd.set_option('display.max_columns', 100)\", '####################\\n# Loading the Data #\\n######...\\ncolId = pd.read_csv(\"test.csv\")\\ncolId = colId.Id', '####################################\\n# Applying ...[c] = le.fit_transform(test_clean[c].astype(str))', '# train_clean.head()\\n# test_clean.head()', '##################\\n# Splitting Data #  #(Only sp...\", axis = 1)\\nY_full_train = train_clean.SalePrice', '################################################...nd(mean_squared_error(Y_test, forestPredictions))', '################################################...nd(mean_squared_error(Y_test, forestPredictions))', \"############################################\\n# J...t = k\\n        index = i\\nprint(index, ':', lowest)\", '############################################\\n# R...\\n# forestPredictions = randForest.predict(X_test)', '################################################...sv\", index = False)\\nprint(KagglePredictions[0:5])', '################################\\n# Setting Up Gr...tures\": (1,65),\\n    \\'min_samples_leaf\\': (5,10)\\n})', 'gradBoostBaye.maximize(n_iter=30)'], 'KagglePredictions': [122753.72779751537, 152625.27336808146, 178481.14250168705, 182418.27386204046, 197268.41760919691, 182664.18188203426, 167298.51146945212, 174886.84548749606, 183808.1585006878, 120455.06351591121, 201345.0218620052, 94250.893415015278, 95141.235855385632, 153242.18828420466, 143054.4601774269, 366644.82223426033, 249802.78058747252, 306951.6298913387, 275113.51871903281, 441520.28016266105, ...], 'LabelEncoder': <class 'sklearn.preprocessing.label.LabelEncoder'>, 'Out': {10: RandomForestRegressor(bootstrap=True, criterion=...state=42,\n           verbose=0, warm_start=False)}, 'RandomForestRegressor': <class 'sklearn.ensemble.forest.RandomForestRegressor'>, 'StandardScaler': <class 'sklearn.preprocessing.data.StandardScaler'>, 'XGBRegressor': <class 'xgboost.sklearn.XGBRegressor'>, ...}\n        self.user_ns = {'BayesianOptimization': <class 'bayes_opt.bayesian_optimization.BayesianOptimization'>, 'ElasticNet': <class 'sklearn.linear_model.coordinate_descent.ElasticNet'>, 'GradientBoostingRegressor': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>, 'In': ['', '#######################\\n# Importing Libraries #\\n...ion--#\\nfrom bayes_opt import BayesianOptimization', \"pd.set_option('display.max_columns', 100)\", '####################\\n# Loading the Data #\\n######...\\ncolId = pd.read_csv(\"test.csv\")\\ncolId = colId.Id', '####################################\\n# Applying ...[c] = le.fit_transform(test_clean[c].astype(str))', '# train_clean.head()\\n# test_clean.head()', '##################\\n# Splitting Data #  #(Only sp...\", axis = 1)\\nY_full_train = train_clean.SalePrice', '################################################...nd(mean_squared_error(Y_test, forestPredictions))', '################################################...nd(mean_squared_error(Y_test, forestPredictions))', \"############################################\\n# J...t = k\\n        index = i\\nprint(index, ':', lowest)\", '############################################\\n# R...\\n# forestPredictions = randForest.predict(X_test)', '################################################...sv\", index = False)\\nprint(KagglePredictions[0:5])', '################################\\n# Setting Up Gr...tures\": (1,65),\\n    \\'min_samples_leaf\\': (5,10)\\n})', 'gradBoostBaye.maximize(n_iter=30)'], 'KagglePredictions': [122753.72779751537, 152625.27336808146, 178481.14250168705, 182418.27386204046, 197268.41760919691, 182664.18188203426, 167298.51146945212, 174886.84548749606, 183808.1585006878, 120455.06351591121, 201345.0218620052, 94250.893415015278, 95141.235855385632, 153242.18828420466, 143054.4601774269, 366644.82223426033, 249802.78058747252, 306951.6298913387, 275113.51871903281, 441520.28016266105, ...], 'LabelEncoder': <class 'sklearn.preprocessing.label.LabelEncoder'>, 'Out': {10: RandomForestRegressor(bootstrap=True, criterion=...state=42,\n           verbose=0, warm_start=False)}, 'RandomForestRegressor': <class 'sklearn.ensemble.forest.RandomForestRegressor'>, 'StandardScaler': <class 'sklearn.preprocessing.data.StandardScaler'>, 'XGBRegressor': <class 'xgboost.sklearn.XGBRegressor'>, ...}\n   2863             finally:\n   2864                 # Reset our crash handler in place\n   2865                 sys.excepthook = old_excepthook\n   2866         except SystemExit as e:\n\n...........................................................................\n/Users/nicholasmaloof/CodingProjects/KaggleProject/<ipython-input-13-1608e59afc68> in <module>()\n----> 1 gradBoostBaye.maximize(n_iter=30)\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py in maximize(self=<bayes_opt.bayesian_optimization.BayesianOptimization object>, init_points=5, n_iter=30, acq='ucb', kappa=2.576, xi=0.0, **gp_params={})\n    306 \n    307                 pwarning = True\n    308 \n    309             # Append most recently generated values to X and Y arrays\n    310             self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n--> 311             self.Y = np.append(self.Y, self.f(**dict(zip(self.keys, x_max))))\n        self.Y = array([-0.01740391, -0.01879441, -0.01728932, -0...01918117, -0.0178101 , -0.01677769, -0.02061838])\n        self.f = <function gradBoostCV>\n        self.keys = ['n_estimators', 'max_depth', 'max_features', 'min_samples_leaf']\n        x_max = array([ 599.17521686,   15.        ,   65.        ,   10.        ])\n    312 \n    313             # Updating the GP.\n    314             ur = unique_rows(self.X)\n    315             self.gp.fit(self.X[ur], self.Y[ur])\n\n...........................................................................\n/Users/nicholasmaloof/CodingProjects/KaggleProject/<ipython-input-12-ffa43c0e7bd4> in gradBoostCV(n_estimators=599.17521686456018, max_depth=15.0, max_features=65.0, min_samples_leaf=10.0)\n      4 \n      5 def gradBoostCV(n_estimators, max_depth, max_features, min_samples_leaf):\n      6     val = cross_val_score(GradientBoostingRegressor(\n      7     n_estimators = int(n_estimators), max_depth = int(max_depth), max_features = int(max_features), min_samples_leaf = int(min_samples_leaf),\n      8         random_state = 42, learning_rate = 0.05\n----> 9     ),X_train, Y_train, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = 3).mean()\n     10     return val\n     11 \n     12 gradBoostBaye = BayesianOptimization(gradBoostCV, {\n     13     'n_estimators': (100, 10000),\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in cross_val_score(estimator=GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False), X=      MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1168 rows x 64 columns], y=254     11.884496\n1066    12.089544\n638     11.3...816\nName: SalePrice, Length: 1168, dtype: float64, groups=None, scoring='neg_mean_squared_error', cv=10, n_jobs=3, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')\n    337     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n    338                                 scoring={'score': scorer}, cv=cv,\n    339                                 return_train_score=False,\n    340                                 n_jobs=n_jobs, verbose=verbose,\n    341                                 fit_params=fit_params,\n--> 342                                 pre_dispatch=pre_dispatch)\n        pre_dispatch = '2*n_jobs'\n    343     return cv_results['test_score']\n    344 \n    345 \n    346 def _fit_and_score(estimator, X, y, scorer, train, test, verbose,\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in cross_validate(estimator=GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False), X=      MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1168 rows x 64 columns], y=254     11.884496\n1066    12.089544\n638     11.3...816\nName: SalePrice, Length: 1168, dtype: float64, groups=None, scoring={'score': make_scorer(mean_squared_error, greater_is_better=False)}, cv=KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=3, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', return_train_score=False)\n    201     scores = parallel(\n    202         delayed(_fit_and_score)(\n    203             clone(estimator), X, y, scorers, train, test, verbose, None,\n    204             fit_params, return_train_score=return_train_score,\n    205             return_times=True)\n--> 206         for train, test in cv.split(X, y, groups))\n        cv.split = <bound method _BaseKFold.split of KFold(n_splits=10, random_state=None, shuffle=False)>\n        X =       MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1168 rows x 64 columns]\n        y = 254     11.884496\n1066    12.089544\n638     11.3...816\nName: SalePrice, Length: 1168, dtype: float64\n        groups = None\n    207 \n    208     if return_train_score:\n    209         train_scores, test_scores, fit_times, score_times = zip(*scores)\n    210         train_scores = _aggregate_score_dicts(train_scores)\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=3), iterable=<generator object cross_validate.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=3)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Sat Nov 11 13:53:35 2017\nPID: 3285           Python 3.6.3: /Users/nicholasmaloof/anaconda/bin/python\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False),       MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1168 rows x 64 columns], 254     11.884496\n1066    12.089544\n638     11.3...816\nName: SalePrice, Length: 1168, dtype: float64, {'score': make_scorer(mean_squared_error, greater_is_better=False)}, array([ 117,  118,  119, ..., 1165, 1166, 1167]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...07, 108, 109, 110, 111, 112, 113, 114, 115, 116]), 0, None, None), {'return_times': True, 'return_train_score': False})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False),       MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1168 rows x 64 columns], 254     11.884496\n1066    12.089544\n638     11.3...816\nName: SalePrice, Length: 1168, dtype: float64, {'score': make_scorer(mean_squared_error, greater_is_better=False)}, array([ 117,  118,  119, ..., 1165, 1166, 1167]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...07, 108, 109, 110, 111, 112, 113, 114, 115, 116]), 0, None, None)\n        kwargs = {'return_times': True, 'return_train_score': False}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False), X=      MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1168 rows x 64 columns], y=254     11.884496\n1066    12.089544\n638     11.3...816\nName: SalePrice, Length: 1168, dtype: float64, scorer={'score': make_scorer(mean_squared_error, greater_is_better=False)}, train=array([ 117,  118,  119, ..., 1165, 1166, 1167]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ...07, 108, 109, 110, 111, 112, 113, 114, 115, 116]), verbose=0, parameters=None, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=False, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseGradientBoosting.fit of Gradie...     subsample=1.0, verbose=0, warm_start=False)>\n        X_train =       MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1051 rows x 64 columns]\n        y_train = 1157    12.345839\n1322    12.154785\n704     12.2...816\nName: SalePrice, Length: 1051, dtype: float64\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py in fit(self=GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False), X=array([[  120.        ,     3.        ,     3.55...     8.        ,     4.        ]], dtype=float32), y=array([ 12.34583894,  12.15478461,  12.26905214,... 11.6526961 ,\n        12.15452142,  12.06681633]), sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32), monitor=None)\n   1029                 X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),\n   1030                                                  dtype=np.int32)\n   1031 \n   1032         # fit the boosting stages\n   1033         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n-> 1034                                     begin_at_stage, monitor, X_idx_sorted)\n        begin_at_stage = 0\n        monitor = None\n        X_idx_sorted = array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32)\n   1035         # change shape of arrays after fit (early-stopping or additional ests)\n   1036         if n_stages != self.estimators_.shape[0]:\n   1037             self.estimators_ = self.estimators_[:n_stages]\n   1038             self.train_score_ = self.train_score_[:n_stages]\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py in _fit_stages(self=GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False), X=array([[  120.        ,     3.        ,     3.55...     8.        ,     4.        ]], dtype=float32), y=array([ 12.34583894,  12.15478461,  12.26905214,... 11.6526961 ,\n        12.15452142,  12.06681633]), y_pred=array([[ 12.03199433],\n       [ 12.03199433],\n  ...],\n       [ 12.03199433],\n       [ 12.03199433]]), sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32), random_state=<mtrand.RandomState object>, begin_at_stage=0, monitor=None, X_idx_sorted=array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32))\n   1084                                       sample_weight[~sample_mask])\n   1085 \n   1086             # fit next stage of trees\n   1087             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n   1088                                      sample_mask, random_state, X_idx_sorted,\n-> 1089                                      X_csc, X_csr)\n        X_csc = None\n        X_csr = None\n   1090 \n   1091             # track deviance (= loss)\n   1092             if do_oob:\n   1093                 self.train_score_[i] = loss_(y[sample_mask],\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py in _fit_stage(self=GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False), i=0, X=array([[  120.        ,     3.        ,     3.55...     8.        ,     4.        ]], dtype=float32), y=array([ 12.34583894,  12.15478461,  12.26905214,... 11.6526961 ,\n        12.15452142,  12.06681633]), y_pred=array([[ 12.03199433],\n       [ 12.03199433],\n  ...],\n       [ 12.03199433],\n       [ 12.03199433]]), sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32), sample_mask=array([ True,  True,  True, ...,  True,  True,  True], dtype=bool), random_state=<mtrand.RandomState object>, X_idx_sorted=array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32), X_csc=None, X_csr=None)\n    783             if X_csc is not None:\n    784                 tree.fit(X_csc, residual, sample_weight=sample_weight,\n    785                          check_input=False, X_idx_sorted=X_idx_sorted)\n    786             else:\n    787                 tree.fit(X, residual, sample_weight=sample_weight,\n--> 788                          check_input=False, X_idx_sorted=X_idx_sorted)\n        X_idx_sorted = array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32)\n    789 \n    790             # update tree leaves\n    791             if X_csr is not None:\n    792                 loss.update_terminal_regions(tree.tree_, X_csr, y, residual, y_pred,\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py in fit(self=DecisionTreeRegressor(criterion='friedman_mse', ...ject at 0x10cdec2d0>,\n           splitter='best'), X=array([[  120.        ,     3.        ,     3.55...     8.        ,     4.        ]], dtype=float32), y=array([ 0.3138446 ,  0.12279028,  0.23705781, ..., -0.37929823,\n        0.12252709,  0.03482199]), sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32), check_input=False, X_idx_sorted=array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32))\n   1119 \n   1120         super(DecisionTreeRegressor, self).fit(\n   1121             X, y,\n   1122             sample_weight=sample_weight,\n   1123             check_input=check_input,\n-> 1124             X_idx_sorted=X_idx_sorted)\n        X_idx_sorted = array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32)\n   1125         return self\n   1126 \n   1127 \n   1128 class ExtraTreeClassifier(DecisionTreeClassifier):\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py in fit(self=DecisionTreeRegressor(criterion='friedman_mse', ...ject at 0x10cdec2d0>,\n           splitter='best'), X=array([[  120.        ,     3.        ,     3.55...     8.        ,     4.        ]], dtype=float32), y=array([[ 0.3138446 ],\n       [ 0.12279028],\n    ...23],\n       [ 0.12252709],\n       [ 0.03482199]]), sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32), check_input=False, X_idx_sorted=array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32))\n    237         if not 0 <= self.min_weight_fraction_leaf <= 0.5:\n    238             raise ValueError(\"min_weight_fraction_leaf must in [0, 0.5]\")\n    239         if max_depth <= 0:\n    240             raise ValueError(\"max_depth must be greater than zero. \")\n    241         if not (0 < max_features <= self.n_features_):\n--> 242             raise ValueError(\"max_features must be in (0, n_features]\")\n    243         if not isinstance(max_leaf_nodes, (numbers.Integral, np.integer)):\n    244             raise ValueError(\"max_leaf_nodes must be integral number but was \"\n    245                              \"%r\" % max_leaf_nodes)\n    246         if -1 < max_leaf_nodes < 2:\n\nValueError: max_features must be in (0, n_features]\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 350, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 458, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\", line 1034, in fit\n    begin_at_stage, monitor, X_idx_sorted)\n  File \"/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\", line 1089, in _fit_stages\n    X_csc, X_csr)\n  File \"/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\", line 788, in _fit_stage\n    check_input=False, X_idx_sorted=X_idx_sorted)\n  File \"/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py\", line 1124, in fit\n    X_idx_sorted=X_idx_sorted)\n  File \"/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py\", line 242, in fit\n    raise ValueError(\"max_features must be in (0, n_features]\")\nValueError: max_features must be in (0, n_features]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/nicholasmaloof/anaconda/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 359, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nValueError                                         Sat Nov 11 13:53:35 2017\nPID: 3285           Python 3.6.3: /Users/nicholasmaloof/anaconda/bin/python\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False),       MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1168 rows x 64 columns], 254     11.884496\n1066    12.089544\n638     11.3...816\nName: SalePrice, Length: 1168, dtype: float64, {'score': make_scorer(mean_squared_error, greater_is_better=False)}, array([ 117,  118,  119, ..., 1165, 1166, 1167]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...07, 108, 109, 110, 111, 112, 113, 114, 115, 116]), 0, None, None), {'return_times': True, 'return_train_score': False})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False),       MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1168 rows x 64 columns], 254     11.884496\n1066    12.089544\n638     11.3...816\nName: SalePrice, Length: 1168, dtype: float64, {'score': make_scorer(mean_squared_error, greater_is_better=False)}, array([ 117,  118,  119, ..., 1165, 1166, 1167]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...07, 108, 109, 110, 111, 112, 113, 114, 115, 116]), 0, None, None)\n        kwargs = {'return_times': True, 'return_train_score': False}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False), X=      MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1168 rows x 64 columns], y=254     11.884496\n1066    12.089544\n638     11.3...816\nName: SalePrice, Length: 1168, dtype: float64, scorer={'score': make_scorer(mean_squared_error, greater_is_better=False)}, train=array([ 117,  118,  119, ..., 1165, 1166, 1167]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ...07, 108, 109, 110, 111, 112, 113, 114, 115, 116]), verbose=0, parameters=None, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=False, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseGradientBoosting.fit of Gradie...     subsample=1.0, verbose=0, warm_start=False)>\n        X_train =       MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1051 rows x 64 columns]\n        y_train = 1157    12.345839\n1322    12.154785\n704     12.2...816\nName: SalePrice, Length: 1051, dtype: float64\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py in fit(self=GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False), X=array([[  120.        ,     3.        ,     3.55...     8.        ,     4.        ]], dtype=float32), y=array([ 12.34583894,  12.15478461,  12.26905214,... 11.6526961 ,\n        12.15452142,  12.06681633]), sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32), monitor=None)\n   1029                 X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),\n   1030                                                  dtype=np.int32)\n   1031 \n   1032         # fit the boosting stages\n   1033         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n-> 1034                                     begin_at_stage, monitor, X_idx_sorted)\n        begin_at_stage = 0\n        monitor = None\n        X_idx_sorted = array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32)\n   1035         # change shape of arrays after fit (early-stopping or additional ests)\n   1036         if n_stages != self.estimators_.shape[0]:\n   1037             self.estimators_ = self.estimators_[:n_stages]\n   1038             self.train_score_ = self.train_score_[:n_stages]\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py in _fit_stages(self=GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False), X=array([[  120.        ,     3.        ,     3.55...     8.        ,     4.        ]], dtype=float32), y=array([ 12.34583894,  12.15478461,  12.26905214,... 11.6526961 ,\n        12.15452142,  12.06681633]), y_pred=array([[ 12.03199433],\n       [ 12.03199433],\n  ...],\n       [ 12.03199433],\n       [ 12.03199433]]), sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32), random_state=<mtrand.RandomState object>, begin_at_stage=0, monitor=None, X_idx_sorted=array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32))\n   1084                                       sample_weight[~sample_mask])\n   1085 \n   1086             # fit next stage of trees\n   1087             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n   1088                                      sample_mask, random_state, X_idx_sorted,\n-> 1089                                      X_csc, X_csr)\n        X_csc = None\n        X_csr = None\n   1090 \n   1091             # track deviance (= loss)\n   1092             if do_oob:\n   1093                 self.train_score_[i] = loss_(y[sample_mask],\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py in _fit_stage(self=GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False), i=0, X=array([[  120.        ,     3.        ,     3.55...     8.        ,     4.        ]], dtype=float32), y=array([ 12.34583894,  12.15478461,  12.26905214,... 11.6526961 ,\n        12.15452142,  12.06681633]), y_pred=array([[ 12.03199433],\n       [ 12.03199433],\n  ...],\n       [ 12.03199433],\n       [ 12.03199433]]), sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32), sample_mask=array([ True,  True,  True, ...,  True,  True,  True], dtype=bool), random_state=<mtrand.RandomState object>, X_idx_sorted=array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32), X_csc=None, X_csr=None)\n    783             if X_csc is not None:\n    784                 tree.fit(X_csc, residual, sample_weight=sample_weight,\n    785                          check_input=False, X_idx_sorted=X_idx_sorted)\n    786             else:\n    787                 tree.fit(X, residual, sample_weight=sample_weight,\n--> 788                          check_input=False, X_idx_sorted=X_idx_sorted)\n        X_idx_sorted = array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32)\n    789 \n    790             # update tree leaves\n    791             if X_csr is not None:\n    792                 loss.update_terminal_regions(tree.tree_, X_csr, y, residual, y_pred,\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py in fit(self=DecisionTreeRegressor(criterion='friedman_mse', ...ject at 0x10cdec2d0>,\n           splitter='best'), X=array([[  120.        ,     3.        ,     3.55...     8.        ,     4.        ]], dtype=float32), y=array([ 0.3138446 ,  0.12279028,  0.23705781, ..., -0.37929823,\n        0.12252709,  0.03482199]), sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32), check_input=False, X_idx_sorted=array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32))\n   1119 \n   1120         super(DecisionTreeRegressor, self).fit(\n   1121             X, y,\n   1122             sample_weight=sample_weight,\n   1123             check_input=check_input,\n-> 1124             X_idx_sorted=X_idx_sorted)\n        X_idx_sorted = array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32)\n   1125         return self\n   1126 \n   1127 \n   1128 class ExtraTreeClassifier(DecisionTreeClassifier):\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py in fit(self=DecisionTreeRegressor(criterion='friedman_mse', ...ject at 0x10cdec2d0>,\n           splitter='best'), X=array([[  120.        ,     3.        ,     3.55...     8.        ,     4.        ]], dtype=float32), y=array([[ 0.3138446 ],\n       [ 0.12279028],\n    ...23],\n       [ 0.12252709],\n       [ 0.03482199]]), sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32), check_input=False, X_idx_sorted=array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32))\n    237         if not 0 <= self.min_weight_fraction_leaf <= 0.5:\n    238             raise ValueError(\"min_weight_fraction_leaf must in [0, 0.5]\")\n    239         if max_depth <= 0:\n    240             raise ValueError(\"max_depth must be greater than zero. \")\n    241         if not (0 < max_features <= self.n_features_):\n--> 242             raise ValueError(\"max_features must be in (0, n_features]\")\n    243         if not isinstance(max_leaf_nodes, (numbers.Integral, np.integer)):\n    244             raise ValueError(\"max_leaf_nodes must be integral number but was \"\n    245                              \"%r\" % max_leaf_nodes)\n    246         if -1 < max_leaf_nodes < 2:\n\nValueError: max_features must be in (0, n_features]\n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nValueError                                         Sat Nov 11 13:53:35 2017\nPID: 3285           Python 3.6.3: /Users/nicholasmaloof/anaconda/bin/python\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False),       MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1168 rows x 64 columns], 254     11.884496\n1066    12.089544\n638     11.3...816\nName: SalePrice, Length: 1168, dtype: float64, {'score': make_scorer(mean_squared_error, greater_is_better=False)}, array([ 117,  118,  119, ..., 1165, 1166, 1167]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...07, 108, 109, 110, 111, 112, 113, 114, 115, 116]), 0, None, None), {'return_times': True, 'return_train_score': False})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False),       MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1168 rows x 64 columns], 254     11.884496\n1066    12.089544\n638     11.3...816\nName: SalePrice, Length: 1168, dtype: float64, {'score': make_scorer(mean_squared_error, greater_is_better=False)}, array([ 117,  118,  119, ..., 1165, 1166, 1167]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...07, 108, 109, 110, 111, 112, 113, 114, 115, 116]), 0, None, None)\n        kwargs = {'return_times': True, 'return_train_score': False}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False), X=      MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1168 rows x 64 columns], y=254     11.884496\n1066    12.089544\n638     11.3...816\nName: SalePrice, Length: 1168, dtype: float64, scorer={'score': make_scorer(mean_squared_error, greater_is_better=False)}, train=array([ 117,  118,  119, ..., 1165, 1166, 1167]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ...07, 108, 109, 110, 111, 112, 113, 114, 115, 116]), verbose=0, parameters=None, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=False, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseGradientBoosting.fit of Gradie...     subsample=1.0, verbose=0, warm_start=False)>\n        X_train =       MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1051 rows x 64 columns]\n        y_train = 1157    12.345839\n1322    12.154785\n704     12.2...816\nName: SalePrice, Length: 1051, dtype: float64\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py in fit(self=GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False), X=array([[  120.        ,     3.        ,     3.55...     8.        ,     4.        ]], dtype=float32), y=array([ 12.34583894,  12.15478461,  12.26905214,... 11.6526961 ,\n        12.15452142,  12.06681633]), sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32), monitor=None)\n   1029                 X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),\n   1030                                                  dtype=np.int32)\n   1031 \n   1032         # fit the boosting stages\n   1033         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n-> 1034                                     begin_at_stage, monitor, X_idx_sorted)\n        begin_at_stage = 0\n        monitor = None\n        X_idx_sorted = array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32)\n   1035         # change shape of arrays after fit (early-stopping or additional ests)\n   1036         if n_stages != self.estimators_.shape[0]:\n   1037             self.estimators_ = self.estimators_[:n_stages]\n   1038             self.train_score_ = self.train_score_[:n_stages]\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py in _fit_stages(self=GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False), X=array([[  120.        ,     3.        ,     3.55...     8.        ,     4.        ]], dtype=float32), y=array([ 12.34583894,  12.15478461,  12.26905214,... 11.6526961 ,\n        12.15452142,  12.06681633]), y_pred=array([[ 12.03199433],\n       [ 12.03199433],\n  ...],\n       [ 12.03199433],\n       [ 12.03199433]]), sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32), random_state=<mtrand.RandomState object>, begin_at_stage=0, monitor=None, X_idx_sorted=array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32))\n   1084                                       sample_weight[~sample_mask])\n   1085 \n   1086             # fit next stage of trees\n   1087             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n   1088                                      sample_mask, random_state, X_idx_sorted,\n-> 1089                                      X_csc, X_csr)\n        X_csc = None\n        X_csr = None\n   1090 \n   1091             # track deviance (= loss)\n   1092             if do_oob:\n   1093                 self.train_score_[i] = loss_(y[sample_mask],\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py in _fit_stage(self=GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False), i=0, X=array([[  120.        ,     3.        ,     3.55...     8.        ,     4.        ]], dtype=float32), y=array([ 12.34583894,  12.15478461,  12.26905214,... 11.6526961 ,\n        12.15452142,  12.06681633]), y_pred=array([[ 12.03199433],\n       [ 12.03199433],\n  ...],\n       [ 12.03199433],\n       [ 12.03199433]]), sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32), sample_mask=array([ True,  True,  True, ...,  True,  True,  True], dtype=bool), random_state=<mtrand.RandomState object>, X_idx_sorted=array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32), X_csc=None, X_csr=None)\n    783             if X_csc is not None:\n    784                 tree.fit(X_csc, residual, sample_weight=sample_weight,\n    785                          check_input=False, X_idx_sorted=X_idx_sorted)\n    786             else:\n    787                 tree.fit(X, residual, sample_weight=sample_weight,\n--> 788                          check_input=False, X_idx_sorted=X_idx_sorted)\n        X_idx_sorted = array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32)\n    789 \n    790             # update tree leaves\n    791             if X_csr is not None:\n    792                 loss.update_terminal_regions(tree.tree_, X_csr, y, residual, y_pred,\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py in fit(self=DecisionTreeRegressor(criterion='friedman_mse', ...ject at 0x10cdec2d0>,\n           splitter='best'), X=array([[  120.        ,     3.        ,     3.55...     8.        ,     4.        ]], dtype=float32), y=array([ 0.3138446 ,  0.12279028,  0.23705781, ..., -0.37929823,\n        0.12252709,  0.03482199]), sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32), check_input=False, X_idx_sorted=array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32))\n   1119 \n   1120         super(DecisionTreeRegressor, self).fit(\n   1121             X, y,\n   1122             sample_weight=sample_weight,\n   1123             check_input=check_input,\n-> 1124             X_idx_sorted=X_idx_sorted)\n        X_idx_sorted = array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32)\n   1125         return self\n   1126 \n   1127 \n   1128 class ExtraTreeClassifier(DecisionTreeClassifier):\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py in fit(self=DecisionTreeRegressor(criterion='friedman_mse', ...ject at 0x10cdec2d0>,\n           splitter='best'), X=array([[  120.        ,     3.        ,     3.55...     8.        ,     4.        ]], dtype=float32), y=array([[ 0.3138446 ],\n       [ 0.12279028],\n    ...23],\n       [ 0.12252709],\n       [ 0.03482199]]), sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32), check_input=False, X_idx_sorted=array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32))\n    237         if not 0 <= self.min_weight_fraction_leaf <= 0.5:\n    238             raise ValueError(\"min_weight_fraction_leaf must in [0, 0.5]\")\n    239         if max_depth <= 0:\n    240             raise ValueError(\"max_depth must be greater than zero. \")\n    241         if not (0 < max_features <= self.n_features_):\n--> 242             raise ValueError(\"max_features must be in (0, n_features]\")\n    243         if not isinstance(max_leaf_nodes, (numbers.Integral, np.integer)):\n    244             raise ValueError(\"max_leaf_nodes must be integral number but was \"\n    245                              \"%r\" % max_leaf_nodes)\n    246         if -1 < max_leaf_nodes < 2:\n\nValueError: max_features must be in (0, n_features]\n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJoblibValueError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1608e59afc68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgradBoostBaye\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;31m# Append most recently generated values to X and Y arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;31m# Updating the GP.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-ffa43c0e7bd4>\u001b[0m in \u001b[0;36mgradBoostCV\u001b[0;34m(n_estimators, max_depth, max_features, min_samples_leaf)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     ),X_train, Y_train, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = 3).mean()\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             return_times=True)\n\u001b[0;32m--> 206\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    738\u001b[0m                     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJoblibValueError\u001b[0m: JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/runpy.py in _run_code(code=<code object <module> at 0x10497a6f0, file \"/Use...3.6/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/Users/nicholasmaloof/anaconda/lib/python3.6/sit...ges/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/Users/nicho.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x10497a6f0, file \"/Use...3.6/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/Users/nicholasmaloof/anaconda/lib/python3.6/sit...ges/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/Users/nicho.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    472             return self.subapp.start()\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         try:\n--> 477             ioloop.IOLoop.instance().start()\n    478         except KeyboardInterrupt:\n    479             pass\n    480 \n    481 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'gradBoostBaye.maximize(n_iter=30)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2017, 11, 11, 18, 40, 55, 248959, tzinfo=tzutc()), 'msg_id': '74E4C6630D7D4B8D896F0A0982EC0450', 'msg_type': 'execute_request', 'session': '290CB1C368FE44B4B910F9565602A42B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '74E4C6630D7D4B8D896F0A0982EC0450', 'msg_type': 'execute_request', 'parent_header': {}})\n    230             self.log.warn(\"Unknown message type: %r\", msg_type)\n    231         else:\n    232             self.log.debug(\"%s: %s\", msg_type, msg)\n    233             self.pre_handler_hook()\n    234             try:\n--> 235                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'290CB1C368FE44B4B910F9565602A42B']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'gradBoostBaye.maximize(n_iter=30)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2017, 11, 11, 18, 40, 55, 248959, tzinfo=tzutc()), 'msg_id': '74E4C6630D7D4B8D896F0A0982EC0450', 'msg_type': 'execute_request', 'session': '290CB1C368FE44B4B910F9565602A42B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '74E4C6630D7D4B8D896F0A0982EC0450', 'msg_type': 'execute_request', 'parent_header': {}}\n    236             except Exception:\n    237                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    238             finally:\n    239                 self.post_handler_hook()\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'290CB1C368FE44B4B910F9565602A42B'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'gradBoostBaye.maximize(n_iter=30)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2017, 11, 11, 18, 40, 55, 248959, tzinfo=tzutc()), 'msg_id': '74E4C6630D7D4B8D896F0A0982EC0450', 'msg_type': 'execute_request', 'session': '290CB1C368FE44B4B910F9565602A42B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '74E4C6630D7D4B8D896F0A0982EC0450', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='gradBoostBaye.maximize(n_iter=30)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'gradBoostBaye.maximize(n_iter=30)'\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('gradBoostBaye.maximize(n_iter=30)',), **kwargs={'silent': False, 'store_history': True})\n    528             )\n    529         self.payload_manager.write_payload(payload)\n    530 \n    531     def run_cell(self, *args, **kwargs):\n    532         self._last_traceback = None\n--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('gradBoostBaye.maximize(n_iter=30)',)\n        kwargs = {'silent': False, 'store_history': True}\n    534 \n    535     def _showtraceback(self, etype, evalue, stb):\n    536         # try to preserve ordering of tracebacks and print statements\n    537         sys.stdout.flush()\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='gradBoostBaye.maximize(n_iter=30)', store_history=True, silent=False, shell_futures=True)\n   2693                 self.displayhook.exec_result = result\n   2694 \n   2695                 # Execute the user code\n   2696                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2697                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2698                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2699                 \n   2700                 self.last_execution_succeeded = not has_raised\n   2701 \n   2702                 # Reset this so later displayed values do not modify the\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Expr object>], cell_name='<ipython-input-13-1608e59afc68>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 10cf7c358, execution_..._before_exec=None error_in_exec=None result=None>)\n   2803                     return True\n   2804 \n   2805             for i, node in enumerate(to_run_interactive):\n   2806                 mod = ast.Interactive([node])\n   2807                 code = compiler(mod, cell_name, \"single\")\n-> 2808                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x10bbeae40, file \"<ipython-input-13-1608e59afc68>\", line 1>\n        result = <ExecutionResult object at 10cf7c358, execution_..._before_exec=None error_in_exec=None result=None>\n   2809                     return True\n   2810 \n   2811             # Flush softspace\n   2812             if softspace(sys.stdout, 0):\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x10bbeae40, file \"<ipython-input-13-1608e59afc68>\", line 1>, result=<ExecutionResult object at 10cf7c358, execution_..._before_exec=None error_in_exec=None result=None>)\n   2857         outflag = True  # happens in more places, so it's easier as default\n   2858         try:\n   2859             try:\n   2860                 self.hooks.pre_run_code_hook()\n   2861                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2862                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x10bbeae40, file \"<ipython-input-13-1608e59afc68>\", line 1>\n        self.user_global_ns = {'BayesianOptimization': <class 'bayes_opt.bayesian_optimization.BayesianOptimization'>, 'ElasticNet': <class 'sklearn.linear_model.coordinate_descent.ElasticNet'>, 'GradientBoostingRegressor': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>, 'In': ['', '#######################\\n# Importing Libraries #\\n...ion--#\\nfrom bayes_opt import BayesianOptimization', \"pd.set_option('display.max_columns', 100)\", '####################\\n# Loading the Data #\\n######...\\ncolId = pd.read_csv(\"test.csv\")\\ncolId = colId.Id', '####################################\\n# Applying ...[c] = le.fit_transform(test_clean[c].astype(str))', '# train_clean.head()\\n# test_clean.head()', '##################\\n# Splitting Data #  #(Only sp...\", axis = 1)\\nY_full_train = train_clean.SalePrice', '################################################...nd(mean_squared_error(Y_test, forestPredictions))', '################################################...nd(mean_squared_error(Y_test, forestPredictions))', \"############################################\\n# J...t = k\\n        index = i\\nprint(index, ':', lowest)\", '############################################\\n# R...\\n# forestPredictions = randForest.predict(X_test)', '################################################...sv\", index = False)\\nprint(KagglePredictions[0:5])', '################################\\n# Setting Up Gr...tures\": (1,65),\\n    \\'min_samples_leaf\\': (5,10)\\n})', 'gradBoostBaye.maximize(n_iter=30)'], 'KagglePredictions': [122753.72779751537, 152625.27336808146, 178481.14250168705, 182418.27386204046, 197268.41760919691, 182664.18188203426, 167298.51146945212, 174886.84548749606, 183808.1585006878, 120455.06351591121, 201345.0218620052, 94250.893415015278, 95141.235855385632, 153242.18828420466, 143054.4601774269, 366644.82223426033, 249802.78058747252, 306951.6298913387, 275113.51871903281, 441520.28016266105, ...], 'LabelEncoder': <class 'sklearn.preprocessing.label.LabelEncoder'>, 'Out': {10: RandomForestRegressor(bootstrap=True, criterion=...state=42,\n           verbose=0, warm_start=False)}, 'RandomForestRegressor': <class 'sklearn.ensemble.forest.RandomForestRegressor'>, 'StandardScaler': <class 'sklearn.preprocessing.data.StandardScaler'>, 'XGBRegressor': <class 'xgboost.sklearn.XGBRegressor'>, ...}\n        self.user_ns = {'BayesianOptimization': <class 'bayes_opt.bayesian_optimization.BayesianOptimization'>, 'ElasticNet': <class 'sklearn.linear_model.coordinate_descent.ElasticNet'>, 'GradientBoostingRegressor': <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>, 'In': ['', '#######################\\n# Importing Libraries #\\n...ion--#\\nfrom bayes_opt import BayesianOptimization', \"pd.set_option('display.max_columns', 100)\", '####################\\n# Loading the Data #\\n######...\\ncolId = pd.read_csv(\"test.csv\")\\ncolId = colId.Id', '####################################\\n# Applying ...[c] = le.fit_transform(test_clean[c].astype(str))', '# train_clean.head()\\n# test_clean.head()', '##################\\n# Splitting Data #  #(Only sp...\", axis = 1)\\nY_full_train = train_clean.SalePrice', '################################################...nd(mean_squared_error(Y_test, forestPredictions))', '################################################...nd(mean_squared_error(Y_test, forestPredictions))', \"############################################\\n# J...t = k\\n        index = i\\nprint(index, ':', lowest)\", '############################################\\n# R...\\n# forestPredictions = randForest.predict(X_test)', '################################################...sv\", index = False)\\nprint(KagglePredictions[0:5])', '################################\\n# Setting Up Gr...tures\": (1,65),\\n    \\'min_samples_leaf\\': (5,10)\\n})', 'gradBoostBaye.maximize(n_iter=30)'], 'KagglePredictions': [122753.72779751537, 152625.27336808146, 178481.14250168705, 182418.27386204046, 197268.41760919691, 182664.18188203426, 167298.51146945212, 174886.84548749606, 183808.1585006878, 120455.06351591121, 201345.0218620052, 94250.893415015278, 95141.235855385632, 153242.18828420466, 143054.4601774269, 366644.82223426033, 249802.78058747252, 306951.6298913387, 275113.51871903281, 441520.28016266105, ...], 'LabelEncoder': <class 'sklearn.preprocessing.label.LabelEncoder'>, 'Out': {10: RandomForestRegressor(bootstrap=True, criterion=...state=42,\n           verbose=0, warm_start=False)}, 'RandomForestRegressor': <class 'sklearn.ensemble.forest.RandomForestRegressor'>, 'StandardScaler': <class 'sklearn.preprocessing.data.StandardScaler'>, 'XGBRegressor': <class 'xgboost.sklearn.XGBRegressor'>, ...}\n   2863             finally:\n   2864                 # Reset our crash handler in place\n   2865                 sys.excepthook = old_excepthook\n   2866         except SystemExit as e:\n\n...........................................................................\n/Users/nicholasmaloof/CodingProjects/KaggleProject/<ipython-input-13-1608e59afc68> in <module>()\n----> 1 gradBoostBaye.maximize(n_iter=30)\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py in maximize(self=<bayes_opt.bayesian_optimization.BayesianOptimization object>, init_points=5, n_iter=30, acq='ucb', kappa=2.576, xi=0.0, **gp_params={})\n    306 \n    307                 pwarning = True\n    308 \n    309             # Append most recently generated values to X and Y arrays\n    310             self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n--> 311             self.Y = np.append(self.Y, self.f(**dict(zip(self.keys, x_max))))\n        self.Y = array([-0.01740391, -0.01879441, -0.01728932, -0...01918117, -0.0178101 , -0.01677769, -0.02061838])\n        self.f = <function gradBoostCV>\n        self.keys = ['n_estimators', 'max_depth', 'max_features', 'min_samples_leaf']\n        x_max = array([ 599.17521686,   15.        ,   65.        ,   10.        ])\n    312 \n    313             # Updating the GP.\n    314             ur = unique_rows(self.X)\n    315             self.gp.fit(self.X[ur], self.Y[ur])\n\n...........................................................................\n/Users/nicholasmaloof/CodingProjects/KaggleProject/<ipython-input-12-ffa43c0e7bd4> in gradBoostCV(n_estimators=599.17521686456018, max_depth=15.0, max_features=65.0, min_samples_leaf=10.0)\n      4 \n      5 def gradBoostCV(n_estimators, max_depth, max_features, min_samples_leaf):\n      6     val = cross_val_score(GradientBoostingRegressor(\n      7     n_estimators = int(n_estimators), max_depth = int(max_depth), max_features = int(max_features), min_samples_leaf = int(min_samples_leaf),\n      8         random_state = 42, learning_rate = 0.05\n----> 9     ),X_train, Y_train, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = 3).mean()\n     10     return val\n     11 \n     12 gradBoostBaye = BayesianOptimization(gradBoostCV, {\n     13     'n_estimators': (100, 10000),\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in cross_val_score(estimator=GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False), X=      MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1168 rows x 64 columns], y=254     11.884496\n1066    12.089544\n638     11.3...816\nName: SalePrice, Length: 1168, dtype: float64, groups=None, scoring='neg_mean_squared_error', cv=10, n_jobs=3, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')\n    337     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n    338                                 scoring={'score': scorer}, cv=cv,\n    339                                 return_train_score=False,\n    340                                 n_jobs=n_jobs, verbose=verbose,\n    341                                 fit_params=fit_params,\n--> 342                                 pre_dispatch=pre_dispatch)\n        pre_dispatch = '2*n_jobs'\n    343     return cv_results['test_score']\n    344 \n    345 \n    346 def _fit_and_score(estimator, X, y, scorer, train, test, verbose,\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in cross_validate(estimator=GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False), X=      MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1168 rows x 64 columns], y=254     11.884496\n1066    12.089544\n638     11.3...816\nName: SalePrice, Length: 1168, dtype: float64, groups=None, scoring={'score': make_scorer(mean_squared_error, greater_is_better=False)}, cv=KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=3, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', return_train_score=False)\n    201     scores = parallel(\n    202         delayed(_fit_and_score)(\n    203             clone(estimator), X, y, scorers, train, test, verbose, None,\n    204             fit_params, return_train_score=return_train_score,\n    205             return_times=True)\n--> 206         for train, test in cv.split(X, y, groups))\n        cv.split = <bound method _BaseKFold.split of KFold(n_splits=10, random_state=None, shuffle=False)>\n        X =       MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1168 rows x 64 columns]\n        y = 254     11.884496\n1066    12.089544\n638     11.3...816\nName: SalePrice, Length: 1168, dtype: float64\n        groups = None\n    207 \n    208     if return_train_score:\n    209         train_scores, test_scores, fit_times, score_times = zip(*scores)\n    210         train_scores = _aggregate_score_dicts(train_scores)\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=3), iterable=<generator object cross_validate.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=3)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Sat Nov 11 13:53:35 2017\nPID: 3285           Python 3.6.3: /Users/nicholasmaloof/anaconda/bin/python\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False),       MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1168 rows x 64 columns], 254     11.884496\n1066    12.089544\n638     11.3...816\nName: SalePrice, Length: 1168, dtype: float64, {'score': make_scorer(mean_squared_error, greater_is_better=False)}, array([ 117,  118,  119, ..., 1165, 1166, 1167]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...07, 108, 109, 110, 111, 112, 113, 114, 115, 116]), 0, None, None), {'return_times': True, 'return_train_score': False})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False),       MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1168 rows x 64 columns], 254     11.884496\n1066    12.089544\n638     11.3...816\nName: SalePrice, Length: 1168, dtype: float64, {'score': make_scorer(mean_squared_error, greater_is_better=False)}, array([ 117,  118,  119, ..., 1165, 1166, 1167]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...07, 108, 109, 110, 111, 112, 113, 114, 115, 116]), 0, None, None)\n        kwargs = {'return_times': True, 'return_train_score': False}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False), X=      MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1168 rows x 64 columns], y=254     11.884496\n1066    12.089544\n638     11.3...816\nName: SalePrice, Length: 1168, dtype: float64, scorer={'score': make_scorer(mean_squared_error, greater_is_better=False)}, train=array([ 117,  118,  119, ..., 1165, 1166, 1167]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ...07, 108, 109, 110, 111, 112, 113, 114, 115, 116]), verbose=0, parameters=None, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=False, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseGradientBoosting.fit of Gradie...     subsample=1.0, verbose=0, warm_start=False)>\n        X_train =       MSSubClass  MSZoning  LotFrontage    LotAr...     8              4  \n\n[1051 rows x 64 columns]\n        y_train = 1157    12.345839\n1322    12.154785\n704     12.2...816\nName: SalePrice, Length: 1051, dtype: float64\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py in fit(self=GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False), X=array([[  120.        ,     3.        ,     3.55...     8.        ,     4.        ]], dtype=float32), y=array([ 12.34583894,  12.15478461,  12.26905214,... 11.6526961 ,\n        12.15452142,  12.06681633]), sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32), monitor=None)\n   1029                 X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),\n   1030                                                  dtype=np.int32)\n   1031 \n   1032         # fit the boosting stages\n   1033         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n-> 1034                                     begin_at_stage, monitor, X_idx_sorted)\n        begin_at_stage = 0\n        monitor = None\n        X_idx_sorted = array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32)\n   1035         # change shape of arrays after fit (early-stopping or additional ests)\n   1036         if n_stages != self.estimators_.shape[0]:\n   1037             self.estimators_ = self.estimators_[:n_stages]\n   1038             self.train_score_ = self.train_score_[:n_stages]\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py in _fit_stages(self=GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False), X=array([[  120.        ,     3.        ,     3.55...     8.        ,     4.        ]], dtype=float32), y=array([ 12.34583894,  12.15478461,  12.26905214,... 11.6526961 ,\n        12.15452142,  12.06681633]), y_pred=array([[ 12.03199433],\n       [ 12.03199433],\n  ...],\n       [ 12.03199433],\n       [ 12.03199433]]), sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32), random_state=<mtrand.RandomState object>, begin_at_stage=0, monitor=None, X_idx_sorted=array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32))\n   1084                                       sample_weight[~sample_mask])\n   1085 \n   1086             # fit next stage of trees\n   1087             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n   1088                                      sample_mask, random_state, X_idx_sorted,\n-> 1089                                      X_csc, X_csr)\n        X_csc = None\n        X_csr = None\n   1090 \n   1091             # track deviance (= loss)\n   1092             if do_oob:\n   1093                 self.train_score_[i] = loss_(y[sample_mask],\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py in _fit_stage(self=GradientBoostingRegressor(alpha=0.9, criterion='...      subsample=1.0, verbose=0, warm_start=False), i=0, X=array([[  120.        ,     3.        ,     3.55...     8.        ,     4.        ]], dtype=float32), y=array([ 12.34583894,  12.15478461,  12.26905214,... 11.6526961 ,\n        12.15452142,  12.06681633]), y_pred=array([[ 12.03199433],\n       [ 12.03199433],\n  ...],\n       [ 12.03199433],\n       [ 12.03199433]]), sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32), sample_mask=array([ True,  True,  True, ...,  True,  True,  True], dtype=bool), random_state=<mtrand.RandomState object>, X_idx_sorted=array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32), X_csc=None, X_csr=None)\n    783             if X_csc is not None:\n    784                 tree.fit(X_csc, residual, sample_weight=sample_weight,\n    785                          check_input=False, X_idx_sorted=X_idx_sorted)\n    786             else:\n    787                 tree.fit(X, residual, sample_weight=sample_weight,\n--> 788                          check_input=False, X_idx_sorted=X_idx_sorted)\n        X_idx_sorted = array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32)\n    789 \n    790             # update tree leaves\n    791             if X_csr is not None:\n    792                 loss.update_terminal_regions(tree.tree_, X_csr, y, residual, y_pred,\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py in fit(self=DecisionTreeRegressor(criterion='friedman_mse', ...ject at 0x10cdec2d0>,\n           splitter='best'), X=array([[  120.        ,     3.        ,     3.55...     8.        ,     4.        ]], dtype=float32), y=array([ 0.3138446 ,  0.12279028,  0.23705781, ..., -0.37929823,\n        0.12252709,  0.03482199]), sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32), check_input=False, X_idx_sorted=array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32))\n   1119 \n   1120         super(DecisionTreeRegressor, self).fit(\n   1121             X, y,\n   1122             sample_weight=sample_weight,\n   1123             check_input=check_input,\n-> 1124             X_idx_sorted=X_idx_sorted)\n        X_idx_sorted = array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32)\n   1125         return self\n   1126 \n   1127 \n   1128 class ExtraTreeClassifier(DecisionTreeClassifier):\n\n...........................................................................\n/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py in fit(self=DecisionTreeRegressor(criterion='friedman_mse', ...ject at 0x10cdec2d0>,\n           splitter='best'), X=array([[  120.        ,     3.        ,     3.55...     8.        ,     4.        ]], dtype=float32), y=array([[ 0.3138446 ],\n       [ 0.12279028],\n    ...23],\n       [ 0.12252709],\n       [ 0.03482199]]), sample_weight=array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32), check_input=False, X_idx_sorted=array([[ 525,  652,  633, ...,  372,  594,  525]... 701,  463, ...,  525, 1050,  766]], dtype=int32))\n    237         if not 0 <= self.min_weight_fraction_leaf <= 0.5:\n    238             raise ValueError(\"min_weight_fraction_leaf must in [0, 0.5]\")\n    239         if max_depth <= 0:\n    240             raise ValueError(\"max_depth must be greater than zero. \")\n    241         if not (0 < max_features <= self.n_features_):\n--> 242             raise ValueError(\"max_features must be in (0, n_features]\")\n    243         if not isinstance(max_leaf_nodes, (numbers.Integral, np.integer)):\n    244             raise ValueError(\"max_leaf_nodes must be integral number but was \"\n    245                              \"%r\" % max_leaf_nodes)\n    246         if -1 < max_leaf_nodes < 2:\n\nValueError: max_features must be in (0, n_features]\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "gradBoostBaye.maximize(n_iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results\n",
      "Gradient Boosting:  -0.0162253473943\n",
      "Gradient Boosting:  {'n_estimators': 4955.786114733809, 'max_depth': 3.4540861195985579, 'max_features': 4.1313438057324419, 'min_samples_leaf': 9.0897713162955593}\n"
     ]
    }
   ],
   "source": [
    "print('Final Results')\n",
    "print('Gradient Boosting: ', gradBoostBaye.res['max']['max_val'])\n",
    "print('Gradient Boosting: ', gradBoostBaye.res['max']['max_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13946220192870779"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################\n",
    "# MSE of Running the GradBoost #\n",
    "################################\n",
    "\n",
    "testGradBoost = GradientBoostingRegressor(n_estimators=4955, max_depth=4, max_features=4, min_samples_leaf = 9,\n",
    "                                          random_state=42, learning_rate=0.05)\n",
    "testGradBoost.fit(X_train, Y_train)\n",
    "testGradBoostPredictions = testGradBoost.predict(X_test)\n",
    "mean_squared_error(Y_test, testGradBoostPredictions) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.05, loss='ls', max_depth=4, max_features=22,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=9,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=4955, presort='auto', random_state=42,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################\n",
    "# Running Gradient Boosting #\n",
    "#############################\n",
    "\n",
    "gradBoost = GradientBoostingRegressor(n_estimators=4955, max_depth=4, max_features=22,  min_samples_leaf = 9,\n",
    "                                      random_state=42, learning_rate=0.05)\n",
    "gradBoost.fit(X_full_train, Y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119237.67019066743, 157431.534724375, 193896.96494558742, 205778.63440263114, 180476.02495524238]\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# Predicting The Kaggle DataSet with Gradient Boosting #\n",
    "########################################################\n",
    "\n",
    "KagglePredictionsGradBoost = gradBoost.predict(test_clean)\n",
    "KagglePredictionsGradBoost = [np.exp(x) - 1 for x in KagglePredictionsGradBoost]\n",
    "pd.DataFrame({\"SalePrice\":KagglePredictionsGradBoost, \"Id\": colId}).to_csv(\"KaggleSubmitPythonGradBoost.csv\", index = False)\n",
    "print(KagglePredictionsGradBoost[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XG Boost Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Setting Up XG Boosting #\n",
    "###########################\n",
    "\n",
    "def xgBoostCV(n_estimators, max_depth, gamma, min_child_weight):\n",
    "    val = cross_val_score(XGBRegressor(n_estimators=int(n_estimators), max_depth=int(max_depth), \n",
    "                                      gamma = gamma, min_child_weight = min_child_weight, learning_rate = 0.05),\n",
    "                          X_train, Y_train, scoring = 'neg_mean_squared_error', \n",
    "                          cv = 10, n_jobs = 3).mean()\n",
    "    return val\n",
    "\n",
    "xgBoostBaye = BayesianOptimization(xgBoostCV, {\n",
    "    'n_estimators': (100, 10000),\n",
    "    'max_depth': (1,30),\n",
    "    \"gamma\": (0,50),\n",
    "    'min_child_weight': (1,50)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |     gamma |   max_depth |   min_child_weight |   n_estimators | \n",
      "    1 | 04m00s | \u001b[35m  -0.12223\u001b[0m | \u001b[32m  44.3084\u001b[0m | \u001b[32m    20.1862\u001b[0m | \u001b[32m           12.2260\u001b[0m | \u001b[32m     9863.0253\u001b[0m | \n",
      "    2 | 02m42s | \u001b[35m  -0.12222\u001b[0m | \u001b[32m  45.2506\u001b[0m | \u001b[32m    10.2043\u001b[0m | \u001b[32m           40.3797\u001b[0m | \u001b[32m     9750.3801\u001b[0m | \n",
      "    3 | 01m29s | \u001b[35m  -0.09753\u001b[0m | \u001b[32m  29.0647\u001b[0m | \u001b[32m    19.0929\u001b[0m | \u001b[32m           30.4211\u001b[0m | \u001b[32m     4744.3020\u001b[0m | \n",
      "    4 | 02m51s | \u001b[35m  -0.07659\u001b[0m | \u001b[32m  18.2012\u001b[0m | \u001b[32m    29.1262\u001b[0m | \u001b[32m           20.3197\u001b[0m | \u001b[32m     7473.2435\u001b[0m | \n",
      "    5 | 03m44s |   -0.12223 |   44.5179 |     12.5091 |            13.1526 |      9693.6111 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |     gamma |   max_depth |   min_child_weight |   n_estimators | \n",
      "    6 | 00m09s |   -0.11040 |   33.7040 |     27.5149 |            34.4816 |       100.0999 | \n",
      "    7 | 00m31s |   -0.12720 |   49.5374 |      2.0227 |            45.6430 |      6345.4022 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.80638301e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 50, 'nit': 6, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    8 | 01m09s | \u001b[35m  -0.03661\u001b[0m | \u001b[32m   3.1978\u001b[0m | \u001b[32m    25.6616\u001b[0m | \u001b[32m           12.5850\u001b[0m | \u001b[32m     2179.9953\u001b[0m | \n",
      "    9 | 00m42s |   -0.12632 |   48.2566 |     25.8321 |            48.5558 |      3125.0120 | \n",
      "   10 | 00m19s |   -0.11541 |   39.9005 |     11.7809 |            49.1862 |      1203.0841 | \n",
      "   11 | 02m24s |   -0.04881 |    6.6606 |     24.9172 |            45.9916 |      8396.8808 | \n",
      "   12 | 00m17s |   -0.12582 |   48.1073 |      2.4935 |             7.8392 |      2132.4283 | \n",
      "   13 | 00m32s | \u001b[35m  -0.01956\u001b[0m | \u001b[32m   0.0000\u001b[0m | \u001b[32m    30.0000\u001b[0m | \u001b[32m            1.0000\u001b[0m | \u001b[32m     5493.9072\u001b[0m | \n",
      "   14 | 00m18s |   -0.01956 |    0.0000 |     30.0000 |             1.0000 |       673.3110 | \n",
      "   15 | 00m26s |   -0.01956 |    0.0000 |     30.0000 |             1.0000 |      3792.2051 | \n",
      "   16 | 05m06s |   -0.12720 |   49.5723 |     13.0015 |             2.1166 |      7909.2362 | \n",
      "   17 | 05m38s |   -0.02363 |    0.6367 |     25.1022 |            11.1941 |      8927.4812 | \n",
      "   18 | 00m34s | \u001b[35m  -0.01776\u001b[0m | \u001b[32m   0.0000\u001b[0m | \u001b[32m    30.0000\u001b[0m | \u001b[32m            3.4519\u001b[0m | \u001b[32m     2724.8785\u001b[0m | \n",
      "   19 | 07m45s |   -0.03159 |    2.0590 |     23.8220 |             1.2735 |      6864.3149 | \n",
      "   20 | 01m53s |   -0.02898 |    1.5558 |     28.6846 |            19.7052 |      4111.7020 | \n"
     ]
    }
   ],
   "source": [
    "xgBoostBaye.maximize(n_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results\n",
      "XG Boosting:  -0.0177553442918\n",
      "XG Boosting:  {'n_estimators': 2724.8784928701984, 'max_depth': 29.999999999986429, 'gamma': 0.0, 'min_child_weight': 3.4519264472673363}\n"
     ]
    }
   ],
   "source": [
    "print('Final Results')\n",
    "print('XG Boosting: ', xgBoostBaye.res['max']['max_val'])\n",
    "print('XG Boosting: ', xgBoostBaye.res['max']['max_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13642273111904224"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################\n",
    "# MSE of Running the XG Boost #\n",
    "###############################\n",
    "\n",
    "testXGBoost = XGBRegressor(n_estimators=2724, max_depth=30, gamma=0, min_child_weight = 4, learning_rate=0.05, nthread = 3)\n",
    "testXGBoost.fit(X_train, Y_train)\n",
    "testXGBoostPredictions = testXGBoost.predict(X_test)\n",
    "mean_squared_error(Y_test, testXGBoostPredictions) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "       learning_rate=0.05, max_delta_step=0, max_depth=30,\n",
       "       min_child_weight=4, missing=None, n_estimators=2724, nthread=3,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######################\n",
    "# Running XG Boosting #\n",
    "#######################\n",
    "\n",
    "XGBoost = XGBRegressor(n_estimators=2724, max_depth=30, gamma = 0, min_child_weight=4, learning_rate=0.05, nthread = 3)\n",
    "XGBoost.fit(X_full_train, Y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120856.078125, 161563.9375, 187823.234375, 193709.671875, 179868.703125]\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# Predicting The Kaggle DataSet with XG Boosting #\n",
    "##################################################\n",
    "\n",
    "KagglePredictionsXGBoost = XGBoost.predict(test_clean)\n",
    "KagglePredictionsXGBoost = [np.exp(x) - 1 for x in KagglePredictionsXGBoost]\n",
    "pd.DataFrame({\"SalePrice\":KagglePredictionsXGBoost, \"Id\": colId}).to_csv(\"KaggleSubmitPythonXGBoost.csv\", index = False)\n",
    "print(KagglePredictionsXGBoost[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation For Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dimensions:  (1460, 65)\n",
      "Testing Dimensions:  (1459, 64)\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# Loading the Data #\n",
    "####################\n",
    "\n",
    "train_clean_2 = pd.read_csv(\"train_clean.csv\")\n",
    "test_clean_2 = pd.read_csv(\"test_clean.csv\")\n",
    "\n",
    "print(\"Training Dimensions: \", train_clean_2.shape)\n",
    "print(\"Testing Dimensions: \", test_clean_2.shape)\n",
    "\n",
    "######################\n",
    "# Getting Id Columns #\n",
    "######################\n",
    "colId_2 = pd.read_csv(\"test.csv\")\n",
    "colId_2 = colId_2.Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "# Applying Transforms to Functions #\n",
    "####################################\n",
    "\n",
    "full_one_hot = pd.concat([train_clean_2, test_clean_2])\n",
    "full_one_hot['SalePrice'] = full_one_hot['SalePrice'].apply(lambda x: np.log(x+1))\n",
    "full_one_hot['GarageArea'] = full_one_hot['GarageArea'].apply(lambda x: np.log(x+1))\n",
    "full_one_hot['X2ndFlrSF'] = full_one_hot['X2ndFlrSF'].apply(lambda x: np.log(x+1))\n",
    "full_one_hot['TotalBsmtSF'] = full_one_hot['TotalBsmtSF'].apply(lambda x: np.log(x+1))\n",
    "\n",
    "####################\n",
    "# Random Deletions #\n",
    "####################\n",
    "full_one_hot.drop('GarageArea', axis = 1, inplace=True)\n",
    "full_one_hot.drop('GrLivArea', axis = 1, inplace=True)\n",
    "###########################\n",
    "# End of Random Deletions #\n",
    "###########################\n",
    "\n",
    "full_one_hot = pd.get_dummies(full_one_hot, drop_first=True, dummy_na=True)\n",
    "\n",
    "one_hot_train = full_one_hot[0:1460]\n",
    "one_hot_test = full_one_hot[1460:].drop('SalePrice', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>X1stFlrSF</th>\n",
       "      <th>X2ndFlrSF</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>BldgType_2fmCon</th>\n",
       "      <th>BldgType_Duplex</th>\n",
       "      <th>BldgType_Twnhs</th>\n",
       "      <th>BldgType_TwnhsE</th>\n",
       "      <th>BldgType_nan</th>\n",
       "      <th>BsmtCond_Fa</th>\n",
       "      <th>BsmtCond_Gd</th>\n",
       "      <th>BsmtCond_Po</th>\n",
       "      <th>BsmtCond_TA</th>\n",
       "      <th>BsmtCond_nan</th>\n",
       "      <th>BsmtExposure_Av</th>\n",
       "      <th>BsmtExposure_Gd</th>\n",
       "      <th>BsmtExposure_Mn</th>\n",
       "      <th>BsmtExposure_No</th>\n",
       "      <th>BsmtExposure_nan</th>\n",
       "      <th>BsmtFinType1_Absent</th>\n",
       "      <th>BsmtFinType1_BLQ</th>\n",
       "      <th>BsmtFinType1_GLQ</th>\n",
       "      <th>BsmtFinType1_LwQ</th>\n",
       "      <th>BsmtFinType1_Rec</th>\n",
       "      <th>BsmtFinType1_Unf</th>\n",
       "      <th>...</th>\n",
       "      <th>MasVnrType_nan</th>\n",
       "      <th>Neighborhood_Blueste</th>\n",
       "      <th>Neighborhood_BrDale</th>\n",
       "      <th>Neighborhood_BrkSide</th>\n",
       "      <th>Neighborhood_ClearCr</th>\n",
       "      <th>Neighborhood_CollgCr</th>\n",
       "      <th>Neighborhood_Crawfor</th>\n",
       "      <th>Neighborhood_Edwards</th>\n",
       "      <th>Neighborhood_Gilbert</th>\n",
       "      <th>Neighborhood_IDOTRR</th>\n",
       "      <th>Neighborhood_MeadowV</th>\n",
       "      <th>Neighborhood_Mitchel</th>\n",
       "      <th>Neighborhood_NAmes</th>\n",
       "      <th>Neighborhood_NPkVill</th>\n",
       "      <th>Neighborhood_NWAmes</th>\n",
       "      <th>Neighborhood_NoRidge</th>\n",
       "      <th>Neighborhood_NridgHt</th>\n",
       "      <th>Neighborhood_OldTown</th>\n",
       "      <th>Neighborhood_SWISU</th>\n",
       "      <th>Neighborhood_Sawyer</th>\n",
       "      <th>Neighborhood_SawyerW</th>\n",
       "      <th>Neighborhood_Somerst</th>\n",
       "      <th>Neighborhood_StoneBr</th>\n",
       "      <th>Neighborhood_Timber</th>\n",
       "      <th>Neighborhood_Veenker</th>\n",
       "      <th>Neighborhood_nan</th>\n",
       "      <th>PavedDrive_P</th>\n",
       "      <th>PavedDrive_Y</th>\n",
       "      <th>PavedDrive_nan</th>\n",
       "      <th>RoofStyle_Gable</th>\n",
       "      <th>RoofStyle_Gambrel</th>\n",
       "      <th>RoofStyle_Hip</th>\n",
       "      <th>RoofStyle_Mansard</th>\n",
       "      <th>RoofStyle_Shed</th>\n",
       "      <th>RoofStyle_nan</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <th>SaleCondition_nan</th>\n",
       "      <th>SaleType_CWD</th>\n",
       "      <th>SaleType_Con</th>\n",
       "      <th>SaleType_ConLD</th>\n",
       "      <th>SaleType_ConLI</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleType_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.304449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.568896</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.304449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.304449</td>\n",
       "      <td>6.304449</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>3</td>\n",
       "      <td>5.533389</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.686975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.546974</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.304449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.304449</td>\n",
       "      <td>6.304449</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>4</td>\n",
       "      <td>7.110696</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.903538</td>\n",
       "      <td>5.081404</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7.110696</td>\n",
       "      <td>6.163315</td>\n",
       "      <td>7.110696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1960</td>\n",
       "      <td>1996</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>3</td>\n",
       "      <td>5.823046</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.356108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.253591</td>\n",
       "      <td>4.143135</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>3.496508</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.816736</td>\n",
       "      <td>4.394449</td>\n",
       "      <td>6.878326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>3</td>\n",
       "      <td>6.632002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.476464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.172431</td>\n",
       "      <td>4.317488</td>\n",
       "      <td>60</td>\n",
       "      <td>4.553877</td>\n",
       "      <td>11</td>\n",
       "      <td>3.891820</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>6.904751</td>\n",
       "      <td>5.252273</td>\n",
       "      <td>6.904751</td>\n",
       "      <td>6.912743</td>\n",
       "      <td>1993</td>\n",
       "      <td>1994</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      BedroomAbvGr  BsmtFinSF1  BsmtFullBath  BsmtHalfBath  BsmtUnfSF  \\\n",
       "1454             3    0.000000             0             0   6.304449   \n",
       "1455             3    5.533389             0             0   5.686975   \n",
       "1456             4    7.110696             1             0   0.000000   \n",
       "1457             3    5.823046             0             1   6.356108   \n",
       "1458             3    6.632002             0             0   5.476464   \n",
       "\n",
       "      EnclosedPorch  Fireplaces  FullBath  GarageCars  GarageYrBlt  HalfBath  \\\n",
       "1454            0.0           0         1           0            0         1   \n",
       "1455            0.0           0         1           1            1         1   \n",
       "1456            0.0           1         1           2            1         0   \n",
       "1457            0.0           0         1           0            0         0   \n",
       "1458            0.0           1         2           3            1         1   \n",
       "\n",
       "      KitchenAbvGr   LotArea  LotFrontage  MSSubClass  MasVnrArea  MoSold  \\\n",
       "1454             1  7.568896     3.091042         160    0.000000       6   \n",
       "1455             1  7.546974     3.091042         160    0.000000       4   \n",
       "1456             1  9.903538     5.081404          20    0.000000       9   \n",
       "1457             1  9.253591     4.143135          85    0.000000       7   \n",
       "1458             1  9.172431     4.317488          60    4.553877      11   \n",
       "\n",
       "      OpenPorchSF  OverallCond  OverallQual  ScreenPorch  TotRmsAbvGrd  \\\n",
       "1454     0.000000            7            4          0.0             5   \n",
       "1455     3.218876            5            4          0.0             6   \n",
       "1456     0.000000            7            5          0.0             7   \n",
       "1457     3.496508            5            5          0.0             6   \n",
       "1458     3.891820            5            7          0.0             9   \n",
       "\n",
       "      TotalBsmtSF  WoodDeckSF  X1stFlrSF  X2ndFlrSF  YearBuilt  YearRemodAdd  \\\n",
       "1454     6.304449    0.000000   6.304449   6.304449       1970          1970   \n",
       "1455     6.304449    0.000000   6.304449   6.304449       1970          1970   \n",
       "1456     7.110696    6.163315   7.110696   0.000000       1960          1996   \n",
       "1457     6.816736    4.394449   6.878326   0.000000       1992          1992   \n",
       "1458     6.904751    5.252273   6.904751   6.912743       1993          1994   \n",
       "\n",
       "      YrSold  BldgType_2fmCon  BldgType_Duplex  BldgType_Twnhs  \\\n",
       "1454    2006                0                0               1   \n",
       "1455    2006                0                0               0   \n",
       "1456    2006                0                0               0   \n",
       "1457    2006                0                0               0   \n",
       "1458    2006                0                0               0   \n",
       "\n",
       "      BldgType_TwnhsE  BldgType_nan  BsmtCond_Fa  BsmtCond_Gd  BsmtCond_Po  \\\n",
       "1454                0             0            0            0            0   \n",
       "1455                1             0            0            0            0   \n",
       "1456                0             0            0            0            0   \n",
       "1457                0             0            0            0            0   \n",
       "1458                0             0            0            0            0   \n",
       "\n",
       "      BsmtCond_TA  BsmtCond_nan  BsmtExposure_Av  BsmtExposure_Gd  \\\n",
       "1454            1             0                0                0   \n",
       "1455            1             0                0                0   \n",
       "1456            1             0                0                0   \n",
       "1457            1             0                1                0   \n",
       "1458            1             0                1                0   \n",
       "\n",
       "      BsmtExposure_Mn  BsmtExposure_No  BsmtExposure_nan  BsmtFinType1_Absent  \\\n",
       "1454                0                1                 0                    0   \n",
       "1455                0                1                 0                    0   \n",
       "1456                0                1                 0                    0   \n",
       "1457                0                0                 0                    0   \n",
       "1458                0                0                 0                    0   \n",
       "\n",
       "      BsmtFinType1_BLQ  BsmtFinType1_GLQ  BsmtFinType1_LwQ  BsmtFinType1_Rec  \\\n",
       "1454                 0                 0                 0                 0   \n",
       "1455                 0                 0                 0                 1   \n",
       "1456                 0                 0                 0                 0   \n",
       "1457                 0                 1                 0                 0   \n",
       "1458                 0                 0                 1                 0   \n",
       "\n",
       "      BsmtFinType1_Unf      ...       MasVnrType_nan  Neighborhood_Blueste  \\\n",
       "1454                 1      ...                    0                     0   \n",
       "1455                 0      ...                    0                     0   \n",
       "1456                 0      ...                    0                     0   \n",
       "1457                 0      ...                    0                     0   \n",
       "1458                 0      ...                    0                     0   \n",
       "\n",
       "      Neighborhood_BrDale  Neighborhood_BrkSide  Neighborhood_ClearCr  \\\n",
       "1454                    0                     0                     0   \n",
       "1455                    0                     0                     0   \n",
       "1456                    0                     0                     0   \n",
       "1457                    0                     0                     0   \n",
       "1458                    0                     0                     0   \n",
       "\n",
       "      Neighborhood_CollgCr  Neighborhood_Crawfor  Neighborhood_Edwards  \\\n",
       "1454                     0                     0                     0   \n",
       "1455                     0                     0                     0   \n",
       "1456                     0                     0                     0   \n",
       "1457                     0                     0                     0   \n",
       "1458                     0                     0                     0   \n",
       "\n",
       "      Neighborhood_Gilbert  Neighborhood_IDOTRR  Neighborhood_MeadowV  \\\n",
       "1454                     0                    0                     1   \n",
       "1455                     0                    0                     1   \n",
       "1456                     0                    0                     0   \n",
       "1457                     0                    0                     0   \n",
       "1458                     0                    0                     0   \n",
       "\n",
       "      Neighborhood_Mitchel  Neighborhood_NAmes  Neighborhood_NPkVill  \\\n",
       "1454                     0                   0                     0   \n",
       "1455                     0                   0                     0   \n",
       "1456                     1                   0                     0   \n",
       "1457                     1                   0                     0   \n",
       "1458                     1                   0                     0   \n",
       "\n",
       "      Neighborhood_NWAmes  Neighborhood_NoRidge  Neighborhood_NridgHt  \\\n",
       "1454                    0                     0                     0   \n",
       "1455                    0                     0                     0   \n",
       "1456                    0                     0                     0   \n",
       "1457                    0                     0                     0   \n",
       "1458                    0                     0                     0   \n",
       "\n",
       "      Neighborhood_OldTown  Neighborhood_SWISU  Neighborhood_Sawyer  \\\n",
       "1454                     0                   0                    0   \n",
       "1455                     0                   0                    0   \n",
       "1456                     0                   0                    0   \n",
       "1457                     0                   0                    0   \n",
       "1458                     0                   0                    0   \n",
       "\n",
       "      Neighborhood_SawyerW  Neighborhood_Somerst  Neighborhood_StoneBr  \\\n",
       "1454                     0                     0                     0   \n",
       "1455                     0                     0                     0   \n",
       "1456                     0                     0                     0   \n",
       "1457                     0                     0                     0   \n",
       "1458                     0                     0                     0   \n",
       "\n",
       "      Neighborhood_Timber  Neighborhood_Veenker  Neighborhood_nan  \\\n",
       "1454                    0                     0                 0   \n",
       "1455                    0                     0                 0   \n",
       "1456                    0                     0                 0   \n",
       "1457                    0                     0                 0   \n",
       "1458                    0                     0                 0   \n",
       "\n",
       "      PavedDrive_P  PavedDrive_Y  PavedDrive_nan  RoofStyle_Gable  \\\n",
       "1454             0             1               0                1   \n",
       "1455             0             1               0                1   \n",
       "1456             0             1               0                1   \n",
       "1457             0             1               0                1   \n",
       "1458             0             1               0                1   \n",
       "\n",
       "      RoofStyle_Gambrel  RoofStyle_Hip  RoofStyle_Mansard  RoofStyle_Shed  \\\n",
       "1454                  0              0                  0               0   \n",
       "1455                  0              0                  0               0   \n",
       "1456                  0              0                  0               0   \n",
       "1457                  0              0                  0               0   \n",
       "1458                  0              0                  0               0   \n",
       "\n",
       "      RoofStyle_nan  SaleCondition_AdjLand  SaleCondition_Alloca  \\\n",
       "1454              0                      0                     0   \n",
       "1455              0                      0                     0   \n",
       "1456              0                      0                     0   \n",
       "1457              0                      0                     0   \n",
       "1458              0                      0                     0   \n",
       "\n",
       "      SaleCondition_Family  SaleCondition_Normal  SaleCondition_Partial  \\\n",
       "1454                     0                     1                      0   \n",
       "1455                     0                     0                      0   \n",
       "1456                     0                     0                      0   \n",
       "1457                     0                     1                      0   \n",
       "1458                     0                     1                      0   \n",
       "\n",
       "      SaleCondition_nan  SaleType_CWD  SaleType_Con  SaleType_ConLD  \\\n",
       "1454                  0             0             0               0   \n",
       "1455                  0             0             0               0   \n",
       "1456                  0             0             0               0   \n",
       "1457                  0             0             0               0   \n",
       "1458                  0             0             0               0   \n",
       "\n",
       "      SaleType_ConLI  SaleType_ConLw  SaleType_New  SaleType_Oth  SaleType_WD  \\\n",
       "1454               0               0             0             0            1   \n",
       "1455               0               0             0             0            1   \n",
       "1456               0               0             0             0            1   \n",
       "1457               0               0             0             0            1   \n",
       "1458               0               0             0             0            1   \n",
       "\n",
       "      SaleType_nan  \n",
       "1454             0  \n",
       "1455             0  \n",
       "1456             0  \n",
       "1457             0  \n",
       "1458             0  \n",
       "\n",
       "[5 rows x 247 columns]"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape:  (1168, 248)\n",
      "Test Shape:  (292, 248)\n",
      "Full Shape:  (1460, 247)\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# Splitting Data #  #(Only splitting the training data into two more sets called train_set, and test_set)\n",
    "##################\n",
    "\n",
    "train_set_2, test_set_2 = train_test_split(one_hot_train, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(\"Train Shape: \", train_set_2.shape)\n",
    "print(\"Test Shape: \", test_set_2.shape)\n",
    "\n",
    "X_train_2 = train_set_2.drop(\"SalePrice\", axis = 1)\n",
    "Y_train_2 = train_set_2.SalePrice\n",
    "\n",
    "X_test_2 = test_set_2.drop(\"SalePrice\", axis = 1)\n",
    "Y_test_2 = test_set_2.SalePrice\n",
    "\n",
    "#########################################\n",
    "# The Full Original Training Set to Use #\n",
    "#########################################\n",
    "\n",
    "X_full_train_2 = one_hot_train.drop(\"SalePrice\", axis = 1)\n",
    "Y_full_train_2 = one_hot_train.SalePrice\n",
    "print(\"Full Shape: \", X_full_train_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LinRegCV(alpha, l1_ratio):\n",
    "    val = cross_val_score(make_pipeline(StandardScaler(), ElasticNet(alpha = alpha, l1_ratio = l1_ratio, random_state=42)),\n",
    "                         X_train_2, Y_train_2, scoring = 'neg_mean_squared_error', \n",
    "                          cv = 10, n_jobs = 3).mean()\n",
    "    return val\n",
    "\n",
    "LinRegBaye = BayesianOptimization(LinRegCV,{\n",
    "    'alpha': (0,1),\n",
    "    'l1_ratio': (0,1)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |     alpha |   l1_ratio | \n",
      "    1 | 00m00s | \u001b[35m  -0.04121\u001b[0m | \u001b[32m   0.1090\u001b[0m | \u001b[32m    0.3595\u001b[0m | \n",
      "    2 | 00m00s |   -0.15262 |    0.6839 |     0.8459 | \n",
      "    3 | 00m00s | \u001b[35m  -0.03863\u001b[0m | \u001b[32m   0.0698\u001b[0m | \u001b[32m    0.8896\u001b[0m | \n",
      "    4 | 00m00s |   -0.04688 |    0.1238 |     0.6245 | \n",
      "    5 | 00m00s |   -0.04373 |    0.3291 |     0.0027 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |     alpha |   l1_ratio | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    6 | 00m07s | \u001b[35m  -0.02425\u001b[0m | \u001b[32m   0.0000\u001b[0m | \u001b[32m    0.0000\u001b[0m | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    7 | 00m05s |   -0.02704 |    1.0000 |     0.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    8 | 00m04s |   -0.04618 |    0.0000 |     0.7194 | \n",
      "    9 | 00m02s |   -0.03423 |    0.1675 |     0.2234 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "   10 | 00m02s |   -0.12283 |    1.0000 |     0.2392 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  1.98774976e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 51, 'nit': 6, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   11 | 00m06s |   -0.03227 |    0.7835 |     0.0000 | \n",
      "   12 | 00m04s |   -0.03950 |    0.7878 |     0.0452 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "   13 | 00m03s |   -0.15262 |    0.7543 |     0.6261 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   14 | 00m07s |   -0.03449 |    0.0000 |     1.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   15 | 00m08s |   -0.03284 |    0.0000 |     0.1369 | \n",
      "   16 | 00m04s | \u001b[35m  -0.02309\u001b[0m | \u001b[32m   0.0840\u001b[0m | \u001b[32m    0.0000\u001b[0m | \n",
      "   17 | 00m04s |   -0.09064 |    0.1994 |     1.0000 | \n",
      "   18 | 00m04s |   -0.07713 |    0.3664 |     0.4319 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -4.03730774e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 52, 'nit': 5, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   19 | 00m04s |   -0.15262 |    1.0000 |     1.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20 | 00m07s | \u001b[35m  -0.02137\u001b[0m | \u001b[32m   0.9095\u001b[0m | \u001b[32m    0.0000\u001b[0m | \n",
      "   21 | 00m07s |   -0.03313 |    0.0940 |     0.0622 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -3.56165865e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 51, 'nit': 5, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   22 | 00m06s |   -0.02187 |    0.1707 |     0.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py:250: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -5.72254430e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 49, 'nit': 4, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   23 | 00m08s |   -0.02806 |    0.0000 |     0.2905 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -6.53821535e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 52, 'nit': 5, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   24 | 00m03s |   -0.03024 |    0.8781 |     0.0000 | \n",
      "   25 | 00m04s |   -0.06352 |    0.5694 |     0.2034 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   26 | 00m04s |   -0.12806 |    0.3598 |     0.7460 | \n",
      "   27 | 00m03s |   -0.15262 |    1.0000 |     0.7291 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-1208990.54231111]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-7485363.79580809]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 33, 'nit': 2, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -6.96362195e+08]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-4996774.08787255]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 32, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-188743.58765332]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.05652489e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 54, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -4.00250919e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-41614013.68971011]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -2.94256432e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 90, 'nit': 3, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.19513457e+19]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.04461035e+08]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   28 | 00m03s |   -0.02600 |    0.5829 |     0.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  3.33267954e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.10441203e+21]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-236639.14609556]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 42, 'nit': 2, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-26290191.07257843]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  3.10520455e+08]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-143790.4893647]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 40, 'nit': 2, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  4.62728516e+17]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-303331.38672817]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 36, 'nit': 2, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  5.48499809e+17]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-68985.33564525]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -5.41154149e+20]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.67640991e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 37, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   29 | 00m02s |   -0.03989 |    0.9479 |     0.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ 36218643.73226845]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-603972.02377457]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 38, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  1.18918022e+20]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-629406.71911098]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 32, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  2.81460484e+14]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  5.71612164e+11]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-0.00047724]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 22, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  2.04274452e+22]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -8.64446328e+16]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -4.72013094e+20]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.61288803e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 59, 'nit': 3, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.57834688e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 71, 'nit': 2, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-291742.18346289]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 40, 'nit': 2, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  2.25395003e+12]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.82273480e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 64, 'nit': 2, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   30 | 00m02s |   -0.04033 |    0.6826 |     0.0004 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-7739655.47349954]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -2.45977741e+17]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.06947141e+20]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-267135.09447714]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 40, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -2.42013873e+17]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -9.08219731e+09]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 57, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  2.84810536e+22]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-440269.52502828]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 37, 'nit': 2, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -8.21498537e+22]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -3.99468322e+14]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -4.85189867e+08]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  1.35869994e+13]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.05490744e+18]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -8.10068516e+09]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 74, 'nit': 2, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   31 | 00m03s | \u001b[35m  -0.01980\u001b[0m | \u001b[32m   0.5150\u001b[0m | \u001b[32m    0.0000\u001b[0m | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-28866999.37866902]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.00871938e+21]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -5.92501640e+17]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -2.84094915e+21]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.44333920e+19]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-119654.05456818]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -5.95290042e+15]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.72545145e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 58, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-756128.82172112]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 41, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-452102.84426004]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 32, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  3.71590466e+08]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  1.96707113e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 67, 'nit': 3, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  4.87010526e+21]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-815967.78177762]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   32 | 00m03s |   -0.02612 |    0.5274 |     0.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-56262408.01013184]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -2.83486739e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.74278356e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 51, 'nit': 3, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.81362168e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 33, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-1430397.14031112]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 37, 'nit': 2, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -5.88468526e+20]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -2.30527371e+09]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -8.21577892e+12]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.65610946e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 92, 'nit': 3, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-658263.45883319]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 40, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.04837987e+11]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.25383576e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 35, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  2.88666437e+21]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-853001.44961554]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 39, 'nit': 2, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   33 | 00m02s |   -0.04065 |    0.4518 |     0.0004 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -3.27613050e+08]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -7.12538954e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 41, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-1396486.48359213]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -5.62202166e+20]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-35906554.486516]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -5.32129178e+20]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.01553501e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 50, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -6.37774974e+09]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -9.35021979e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 79, 'nit': 2, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -2.95531443e+11]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 40, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -2.10789755e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 94, 'nit': 2, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -4.66356482e+20]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-10090665.48886144]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 28, 'nit': 2, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -3.68292391e+22]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-2368419.2189883]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 39, 'nit': 2, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   34 | 00m03s |   -0.02764 |    0.4970 |     0.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -3.03441675e+12]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -3.31939127e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 38, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -9.15989262e+16]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  3.20343661e+18]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -5.40588712e+17]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -8.20142435e+15]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -7.32337314e+14]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ 50870395.42492327]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 49, 'nit': 1, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  5.35006811e+13]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -6.16086424e+10]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -4.71966930e+17]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -3.66604262e+17]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 21, 'nit': 0, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  6.77229985e+08]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 63, 'nit': 2, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   35 | 00m02s |   -0.04224 |    0.6167 |     0.0023 | \n"
     ]
    }
   ],
   "source": [
    "LinRegBaye.maximize(n_iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results\n",
      "Linear Regression:  -0.0198029902838\n",
      "Linear Regression:  {'alpha': 0.51496038449599246, 'l1_ratio': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print('Final Results')\n",
    "print('Linear Regression: ', LinRegBaye.res['max']['max_val'])\n",
    "print('Linear Regression: ', LinRegBaye.res['max']['max_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15360478174881523"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################\n",
    "# MSE of Running the Linear Regression #\n",
    "#################### ####################\n",
    "\n",
    "testLinReg = make_pipeline(StandardScaler(), ElasticNet(alpha = 0.51496038449599246, l1_ratio = 0, random_state=42))\n",
    "testLinReg.fit(X_train_2, Y_train_2)\n",
    "testLinRegPredictions = testLinReg.predict(X_test_2)\n",
    "# print(testLinRegPredictions[:20])\n",
    "mean_squared_error(Y_test_2, testLinRegPredictions)**.5\n",
    "\n",
    "# testLinReg = Ridge(random_state=42, tol=0.000000001)\n",
    "# testLinReg.fit(X_train_2, Y_train_2)\n",
    "# testLinRegPredictions = testLinReg.predict(X_test_2)\n",
    "# print(testLinRegPredictions[:20])\n",
    "# print(mean_squared_error(Y_test_2, testLinRegPredictions)**.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasmaloof/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('robustscaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
       "       with_scaling=True)), ('elasticnet', ElasticNet(alpha=0.5149603844959925, copy_X=True, fit_intercept=True,\n",
       "      l1_ratio=0.0, max_iter=1000, normalize=False, positive=False,\n",
       "      precompute=False, random_state=42, selection='cyclic', tol=0.0001,\n",
       "      warm_start=False))])"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################\n",
    "# Running Linear Regression #\n",
    "#############################\n",
    "\n",
    "LinReg = make_pipeline(RobustScaler(), ElasticNet(alpha = 0.51496038449599246, l1_ratio = 0.0, random_state=42))\n",
    "LinReg.fit(X_full_train_2, Y_full_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[134957.96594902061, 171276.65833214589, 187219.00684923012, 205606.12370426315, 172090.75226835298]\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# Predicting The Kaggle DataSet with Linear Regression #\n",
    "########################################################\n",
    "\n",
    "KagglePredictionsLinReg = LinReg.predict(one_hot_test)\n",
    "KagglePredictionsLinReg = [np.exp(x) - 1 for x in KagglePredictionsLinReg]\n",
    "pd.DataFrame({\"SalePrice\":KagglePredictionsLinReg, \"Id\": colId_2}).to_csv(\"KaggleSubmitPythonLinReg.csv\", index = False)\n",
    "print(KagglePredictionsLinReg[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuEAAAIVCAYAAAB/QyGdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a11a6f860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####################################\n",
    "# The Importances of RandomForests #\n",
    "####################################\n",
    "\n",
    "rf_importances = randForest.feature_importances_\n",
    "rf_importances = pd.DataFrame({\"Features\": X_full_train.columns, \"Importances\": rf_importances})\n",
    "rf_importances = rf_importances.sort_values(by = \"Importances\", ascending = False)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.xticks(rotation =45)\n",
    "plt.ylim([0,0.04])\n",
    "\n",
    "barplot_ypos = np.arange(len(rf_importances.Features))\n",
    "plt.bar(barplot_ypos, rf_importances.Importances, align = 'center', alpha = 0.5)\n",
    "plt.xticks(barplot_ypos, rf_importances.Features)\n",
    "plt.ylabel('Relative Importance')\n",
    "plt.title('Importance vs. Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_lowest_features = list(rf_importances.Features.tail().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "blah = np.sum([KagglePredictions, KagglePredictionsGradBoost, KagglePredictionsXGBoost], axis = 0)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\"SalePrice\":blah, \"Id\": colId}).to_csv(\"KaggleSubmitPythonBlah.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_clean_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b99c55611f74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorrelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_clean_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# plot correlation matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# fig = plt.figure()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ax = fig.add_subplot(111)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# cax = ax.matshow(correlations, vmin=-1, vmax=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_clean_2' is not defined"
     ]
    }
   ],
   "source": [
    "correlations = train_clean_2.corr()\n",
    "# plot correlation matrix\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "# fig.colorbar(cax)\n",
    "# ticks = np.arange(0,9,1)\n",
    "# ax.set_xticks(ticks)\n",
    "# ax.set_yticks(ticks)\n",
    "# plt.show()\n",
    "for i in correlations:\n",
    "    for j in range(len(correlations)):\n",
    "        x = abs(correlations[i][j])\n",
    "        if x >= 0.8 and x != 1:\n",
    "            print(i, j, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
